{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lewis\\OneDrive\\Desktop\\Schoolwork\\FYP\\FYProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import layoutparser as lp\n",
    "import pymupdf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPConfig, CLIPTokenizer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified class\n",
    "class DocumentAnalysis():\n",
    "    def __init__(self):\n",
    "        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_R_50_FPN_3x/config', \n",
    "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n",
    "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n",
    "        self.ocr_agent = lp.TesseractAgent(languages='eng') \n",
    "\n",
    "        # Dual encoders for embeddings and retrieval\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(self.tokenizer, chunk_size=77, chunk_overlap=5)\n",
    "\n",
    "        self.dimension = 512  # CLIP's embedding size\n",
    "        self.faiss_index = faiss.IndexFlatL2(self.dimension) # FAISS Vector store\n",
    "        self.metadata_store = {}  # Store mapping of IDs and document page number to content\n",
    "\n",
    "    # Read a PDF document using PyMuPDF\n",
    "    # Returns list of page images in cv2 format\n",
    "    def read_from_path(self, filepath):\n",
    "        doc = pymupdf.open(filepath)\n",
    "        return [self.pixmap_to_cv2(page.get_pixmap(dpi=300)) for page in doc]\n",
    "\n",
    "    # Convert PyMuPDF pixmap to cv2\n",
    "    def pixmap_to_cv2(self, pixmap):\n",
    "        bytes = np.frombuffer(pixmap.samples, dtype=np.uint8)\n",
    "        image = bytes.reshape(pixmap.height, pixmap.width, pixmap.n)\n",
    "        image = image[..., ::-1]\n",
    "        return image\n",
    "\n",
    "    # Takes in image object from read_from_path()\n",
    "    # Detects layout -> Processes ROI by label\n",
    "    def detect_layout(self, image):\n",
    "        layout = self.model.detect(image)\n",
    "\n",
    "        # Separate boxes by category\n",
    "        text_blocks = lp.Layout([b for b in layout if b.type=='Text'])\n",
    "        title_blocks = lp.Layout([b for b in layout if b.type=='Title'])\n",
    "        list_blocks = lp.Layout([b for b in layout if b.type=='List'])\n",
    "        table_blocks = lp.Layout([b for b in layout if b.type=='Table'])\n",
    "        figure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])\n",
    "\n",
    "        # Processing text blocks\n",
    "        # Eliminate text blocks nested in images/figures\n",
    "        text_blocks = lp.Layout([b for b in text_blocks \\\n",
    "                        if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "        # Sort boxes\n",
    "        h, w = image.shape[:2]\n",
    "        left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
    "        left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
    "        left_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "        # The b.coordinates[1] corresponds to the y coordinate of the region\n",
    "        # sort based on that can simulate the top-to-bottom reading order \n",
    "        right_blocks = lp.Layout([b for b in text_blocks if b not in left_blocks])\n",
    "        right_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "\n",
    "        # And finally combine the two lists and add the index\n",
    "        text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
    "\n",
    "        # Perform OCR to extract text\n",
    "        for block in text_blocks + title_blocks + list_blocks + table_blocks:\n",
    "            # Add padding in each image segment to improve robustness\n",
    "            text = self._ocr_on_block(image, block)\n",
    "            block.set(text=text, inplace=True) # Assign parsed text to block element\n",
    "            \n",
    "        # Return all blocks on the page as a list\n",
    "        # Omit titles as it affects retrieval\n",
    "        return text_blocks + list_blocks + table_blocks + figure_blocks\n",
    "\n",
    "    # Function to crop an image given block's bbox and additional padding\n",
    "    def _crop_image(self, image, block, padding=10):\n",
    "        return (block.pad(left=padding, right=padding, top=padding, bottom=padding).crop_image(image))\n",
    "\n",
    "    # Perform OCR to extract text given image and block (for text, tables and lists)\n",
    "    def _ocr_on_block(self, image, block):\n",
    "        # Add padding in each image segment to improve robustness\n",
    "        segment_image = (block.pad(left=5, right=5, top=5, bottom=5).crop_image(image))\n",
    "        return self.ocr_agent.detect(segment_image)\n",
    "\n",
    "    # Vectorstore functions\n",
    "    # Function to chunk text to CLIP max length\n",
    "    def chunk_text(self, text):\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        return chunks\n",
    "\n",
    "    # Function to encode text\n",
    "    def encode_text(self, text):\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_text_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to encode image\n",
    "    def encode_image(self, image):\n",
    "        inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_image_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to add item to FAISS\n",
    "    # Specify content, type, page and bounding box from blocks\n",
    "    def add_to_faiss(self, embedding, content, content_type, page_idx, bbox):\n",
    "        idx = len(self.metadata_store)  # Assign unique index\n",
    "        self.faiss_index.add(embedding)\n",
    "        self.metadata_store[idx] = {\"type\": content_type, \"content\": content, \"page\": page_idx, \"bbox\": bbox}\n",
    "    \n",
    "    # Perform retrieval\n",
    "    # TODO: implement 2 stage retrieval\n",
    "    def search_faiss(self, query, k=10):\n",
    "        query_embedding = self.encode_text(query)\n",
    "        _, indices = self.faiss_index.search(query_embedding, k)\n",
    "        \n",
    "        # Display retrieved items\n",
    "        # retrieved items accessed by metadata_store using fetched indices\n",
    "        for idx in indices[0]:\n",
    "            print(f\"Retrieved {self.metadata_store[idx]['type']}: {self.metadata_store[idx]['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lewis\\OneDrive\\Desktop\\Schoolwork\\FYP\\FYProject\\.venv\\Lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Sample run for 1 document\n",
    "pipeline = DocumentAnalysis()\n",
    "doc_path = \"../data/1706.03762.pdf\"\n",
    "doc = pipeline.read_from_path(doc_path)\n",
    "\n",
    "# Processing for each page\n",
    "# Remove [:3] for entire doc, keep for testing\n",
    "for page_idx, page in enumerate(doc[:3]):\n",
    "    blocks = pipeline.detect_layout(page)\n",
    "\n",
    "    # Processing for each block to be vectorized\n",
    "    for b in blocks:\n",
    "        # Process as an image if detected type is a figure, else process as text\n",
    "        if b.type == \"Figure\":\n",
    "            # Crop and get image embeddings\n",
    "            segmented_image = pipeline._crop_image(page, b, padding=20)\n",
    "            image_embs = pipeline.encode_image(segmented_image)\n",
    "            pipeline.add_to_faiss(embedding=image_embs, content=\"Figure\", content_type=b.type, page_idx=page_idx, bbox=b.block.coordinates)\n",
    "        else:\n",
    "            # Chunk text and create new blocks, and process for each block\n",
    "            # Returns list even if not chunked\n",
    "            chunks = pipeline.chunk_text(b.text)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                b2 = deepcopy(b)\n",
    "                b2.set(text=chunk, inplace=True)\n",
    "\n",
    "                # Create duplicate blocks for each chunk\n",
    "                # Encode using text and add to FAISS\n",
    "                text_embs = pipeline.encode_text(b2.text)\n",
    "                pipeline.add_to_faiss(embedding=text_embs, content=b2.text, content_type=b.type, page_idx=page_idx, bbox=b2.block.coordinates)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Text: The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "Retrieved Text: To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "Retrieved Text: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "Retrieved Title: 3.1 Encoder and Decoder Stacks\n",
      "Retrieved Text: efforts have since continued to push the boundaries of recurrent language models and encoder-decodet\n",
      "architectures [38, 24, 15].\n",
      "Retrieved Text: Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "Retrieved Text: attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "Retrieved Text: Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "Retrieved Text: End-to-end memory networks are based on a recurrent attention mechanism instead of sequence\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "Retrieved Title: Model Architecture\n"
     ]
    }
   ],
   "source": [
    "query=\"How is the encoder used in the Transformer architecture?\"\n",
    "pipeline.search_faiss(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform retrieval\n",
    "# To implement 2 stage retrieval\n",
    "def search_faiss(query, k=3):\n",
    "    query_embedding = encode_text(query)\n",
    "    _, indices = faiss_index.search(query_embedding, k)\n",
    "    \n",
    "    # Display retrieved items\n",
    "    for idx in indices[0]:\n",
    "        print(f\"Retrieved {metadata_store[idx]['type']}: {metadata_store[idx]['content']}\")\n",
    "\n",
    "# Example: Retrieve images using text query\n",
    "search_faiss(\"solar energy\", query_type=\"text\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
