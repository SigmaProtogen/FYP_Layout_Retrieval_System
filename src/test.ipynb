{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "import pymupdf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPConfig, CLIPTokenizer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import voyageai\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# os.environ[\"VOYAGE_API_KEY\"] = \"pa-t-QdSeBOYxYQ83TObGLxkR4iqMZpYylSWOLBmthFUG7\"\n",
    "\n",
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified class for processing, analyzing and storing a document\n",
    "class DocumentAnalysis():\n",
    "    def __init__(self, embedding_model = \"openai/clip-vit-base-patch32\", cross_encoder_model = \"cross-encoder/ms-marco-MiniLM-L6-v2\"):\n",
    "         # Layout detection\n",
    "        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_R_50_FPN_3x/config', \n",
    "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n",
    "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"},\n",
    "                                 device=device)\n",
    "        self.ocr_agent = lp.TesseractAgent(languages='eng') \n",
    "\n",
    "        # Dual encoders for embeddings\n",
    "        self.clip_model = CLIPModel.from_pretrained(embedding_model, device_map=device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(embedding_model)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(embedding_model)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(self.tokenizer, chunk_size=77, chunk_overlap=5)\n",
    "\n",
    "        # Cross encoder for retrieval-reranking\n",
    "        self.cross_encoder_tokenizer = AutoTokenizer.from_pretrained(cross_encoder_model)\n",
    "        self.cross_encoder = AutoModelForSequenceClassification.from_pretrained(cross_encoder_model).to(device)       \n",
    "\n",
    "        # Vectorstore variables\n",
    "        self.dimension = 512  # CLIP's embedding size\n",
    "        self.faiss_index = faiss.IndexFlatL2(self.dimension) # FAISS Vector store\n",
    "        self.metadata_store = {}  # Store mapping of IDs and document page number to content\n",
    "        self.vector_dir = '../data/.vectorstore/' # Directory to write data to\n",
    "\n",
    "    # Read a PDF document using PyMuPDF\n",
    "    # Returns list of page images in cv2 format\n",
    "    def read_from_path(self, filepath):\n",
    "        doc = pymupdf.open(filepath)\n",
    "        return [self.pixmap_to_cv2(page.get_pixmap(dpi=300)) for page in doc]\n",
    "\n",
    "    # Convert PyMuPDF pixmap to cv2\n",
    "    def pixmap_to_cv2(self, pixmap):\n",
    "        bytes = np.frombuffer(pixmap.samples, dtype=np.uint8)\n",
    "        image = bytes.reshape(pixmap.height, pixmap.width, pixmap.n)\n",
    "        image = image[..., ::-1]\n",
    "        return image\n",
    "\n",
    "    # Takes in image object from read_from_path()\n",
    "    # Detects layout -> Processes ROI by label\n",
    "    def detect_layout(self, image):\n",
    "        layout = self.model.detect(image)\n",
    "\n",
    "        # Separate boxes by category\n",
    "        text_blocks = lp.Layout([b for b in layout if b.type=='Text'])\n",
    "        title_blocks = lp.Layout([b for b in layout if b.type=='Title'])\n",
    "        list_blocks = lp.Layout([b for b in layout if b.type=='List'])\n",
    "        table_blocks = lp.Layout([b for b in layout if b.type=='Table'])\n",
    "        figure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])\n",
    "\n",
    "        # Processing text blocks\n",
    "        # Sourced from LayoutParser's Deep Layout Analysis example\n",
    "        # Eliminate text blocks nested in images/figures\n",
    "        text_blocks = lp.Layout([b for b in text_blocks \\\n",
    "                        if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "        # Sort boxes\n",
    "        h, w = image.shape[:2]\n",
    "        left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
    "        left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
    "        left_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "        # The b.coordinates[1] corresponds to the y coordinate of the region\n",
    "        # sort based on that can simulate the top-to-bottom reading order \n",
    "        right_blocks = lp.Layout([b for b in text_blocks if b not in left_blocks])\n",
    "        right_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "\n",
    "        # And finally combine the two lists and add the index\n",
    "        text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
    "\n",
    "        # Perform OCR to extract text\n",
    "        for block in text_blocks + title_blocks + list_blocks + table_blocks + figure_blocks:\n",
    "            # Add padding in each image segment to improve robustness\n",
    "            text = self._ocr_on_block(image, block)\n",
    "            block.set(text=text, inplace=True) # Assign parsed text to block element\n",
    "            \n",
    "        # Return all blocks on the page as a list\n",
    "        return text_blocks + title_blocks + list_blocks + table_blocks + figure_blocks\n",
    "\n",
    "    # Function to crop an image given block's bbox and additional padding\n",
    "    def _crop_image(self, image, block, padding=10):\n",
    "        return (block.pad(left=padding, right=padding, top=padding, bottom=padding).crop_image(image))\n",
    "\n",
    "    # Perform OCR to extract text given image and block (for text, tables and lists)\n",
    "    def _ocr_on_block(self, image, block):\n",
    "        # Add padding in each image segment to improve robustness\n",
    "        segment_image = (block.pad(left=5, right=5, top=5, bottom=5).crop_image(image))\n",
    "        return self.ocr_agent.detect(segment_image)\n",
    "\n",
    "    # Vectorstore functions\n",
    "    # Function to chunk text to CLIP max length\n",
    "    def chunk_text(self, text):\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        return chunks\n",
    "\n",
    "    # Function to encode text\n",
    "    def encode_text(self, text):\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_text_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to encode image\n",
    "    def encode_image(self, image):\n",
    "        inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_image_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to encode both image and text simultaneously\n",
    "    def encode_multimodal(self, image, text=None):\n",
    "        # If no text detected, format to empty list\n",
    "        if text is None or len(text)==0: text=[]\n",
    "        inputs = self.clip_processor(images=image, text=[text], return_tensors='pt')\n",
    "        # Get image embeddings\n",
    "        inputs_image = {'pixel_values': inputs['pixel_values']}\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_image_features(**inputs_image).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "        \n",
    "        \n",
    "    # Function to add item to FAISS\n",
    "    # Specify content, type, page and bounding box from blocks\n",
    "    def add_to_faiss(self, embedding, content, content_type, page_idx, bbox):\n",
    "        idx = len(self.metadata_store)  # Assign unique index\n",
    "        self.faiss_index.add(embedding)\n",
    "        self.metadata_store[idx] = {\"type\": content_type, \"content\": content, \"page\": page_idx, \"bbox\": bbox}\n",
    "    \n",
    "    # Perform retrieval and reranking\n",
    "    def search_faiss(self, query, k=15, n=5):\n",
    "        query_embedding = self.encode_text(query)\n",
    "        _, indices = self.faiss_index.search(query_embedding, k)\n",
    "        indices = [int(i) for i in indices[0]]\n",
    "        \n",
    "        # Cross encoder reranking on text modality\n",
    "        answers = [self.metadata_store[idx] for idx in indices if self.metadata_store[idx]['type']!='Figure']\n",
    "        answer_texts = [a['content'] for a in answers]\n",
    "        queries = [query for i in range(len(answers))] # Repeat for tokenizer input\n",
    "        features = self.cross_encoder_tokenizer(queries, answer_texts,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad(): # Rerank\n",
    "            scores = self.cross_encoder(**features).logits\n",
    "\n",
    "        # Get indices of the top n scores)\n",
    "        best_indices = np.argsort(np.array(scores.flatten()))[-n:][::-1]  # Sort and reverse\n",
    "\n",
    "        # Retrieve responses using the indices\n",
    "        best_answers = [answers[i] for i in best_indices]\n",
    "        return best_answers\n",
    "\n",
    "\n",
    "    # Writes the vectorstore and metadata into a given path\n",
    "    def faiss_persist(self):\n",
    "        faiss.write_index(self.faiss_index, self.vector_dir+\"faiss.index\")\n",
    "        json.dump(self.metadata_store, open(self.vector_dir+\"metadata.json\", 'w'))\n",
    "    \n",
    "    # Read from existing vector stores\n",
    "    def faiss_read(self):\n",
    "        self.faiss_index = faiss.read_index(self.vector_dir+\"faiss.index\")\n",
    "        self.metadata_store = json.load(open(self.vector_dir+\"metadata.json\", 'r'), object_hook=self._convert_keys)\n",
    "    \n",
    "    # Convert keys from string to int when deserializing\n",
    "    def _convert_keys(self, d):\n",
    "        return {int(k) if k.isdigit() else k: v for k, v in d.items()}\n",
    "\n",
    "    # Function to process all pages of a document given all the functions above\n",
    "    # Returns nothing, processes and ingests document into the object's metadata store\n",
    "    def process_document(self, doc):\n",
    "        for page_idx, page in enumerate(tqdm(doc)):\n",
    "            blocks = self.detect_layout(page)\n",
    "\n",
    "            # Processing for each block to be vectorized\n",
    "            for b in blocks:\n",
    "                if b.type == \"Text\":\n",
    "                    # Chunk text and create new blocks, and process for each block\n",
    "                    # Returns list even if unchanged\n",
    "                    chunks = self.chunk_text(b.text)\n",
    "                    for chunk in chunks:\n",
    "                        # Encode as text and add to FAISS\n",
    "                        # Embeddings use the chunks, but metadata contains original text for completion\n",
    "                        text_embs = self.encode_text(chunk)\n",
    "                        self.add_to_faiss(\n",
    "                            embedding=text_embs, \n",
    "                            content=b.text, \n",
    "                            content_type=b.type, \n",
    "                            page_idx=page_idx, \n",
    "                            bbox=b.block.coordinates\n",
    "                        )\n",
    "                else:\n",
    "                    # Multimodal for images and layout\n",
    "                    # Crop and get image embeddings\n",
    "                    segmented_image = self._crop_image(page, b, padding=20)\n",
    "                    multimodal_embs = self.encode_multimodal(segmented_image, b.text)\n",
    "                    self.add_to_faiss(\n",
    "                        embedding=multimodal_embs, \n",
    "                        content=b.text, \n",
    "                        content_type=b.type, \n",
    "                        page_idx=page_idx, \n",
    "                        bbox=b.block.coordinates\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample run for 1 document\n",
    "# For debug, run pipeline.faiss_read() in cell below to prevent rereading doc\n",
    "pipeline = DocumentAnalysis()\n",
    "doc_path = \"../data/1706.03762.pdf\"\n",
    "doc = pipeline.read_from_path(doc_path)\n",
    "pipeline.process_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'Text', 'content': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the\\nyrder of the sequence, we must inject some information about the relative or absolute position of the\\nokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nottoms of the encoder and decoder stacks. The positional encodings have the same dimension dode|\\nis the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nearned and fixed [9].\\n', 'page': 5, 'bbox': [464.0125732421875, 973.0946655273438, 2113.4306640625, 1254.83251953125]}, {'type': 'Text', 'content': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the\\nyrder of the sequence, we must inject some information about the relative or absolute position of the\\nokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nottoms of the encoder and decoder stacks. The positional encodings have the same dimension dode|\\nis the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nearned and fixed [9].\\n', 'page': 5, 'bbox': [464.0125732421875, 973.0946655273438, 2113.4306640625, 1254.83251953125]}, {'type': 'Text', 'content': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\n', 'page': 1, 'bbox': [450.40130615234375, 1614.4937744140625, 2103.35205078125, 2032.34375]}, {'type': 'Text', 'content': 'The Transformer uses multi-head attention in three different ways:\\n', 'page': 4, 'bbox': [457.04034423828125, 1247.1748046875, 1558.388671875, 1290.0250244140625]}, {'type': 'Text', 'content': 'To the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n', 'page': 1, 'bbox': [455.5078430175781, 2412.51611328125, 2113.541015625, 2598.93115234375]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lewis\\AppData\\Local\\Temp\\ipykernel_13468\\3315288809.py:143: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  best_indices = np.argsort(np.array(scores.flatten()))[-n:][::-1]  # Sort and reverse\n"
     ]
    }
   ],
   "source": [
    "# Vectordb search test and persist test\n",
    "pipeline.faiss_persist()\n",
    "query=\"How does a Transformer use positional encoding?\"\n",
    "answer = pipeline.search_faiss(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lewis\\AppData\\Local\\Temp\\ipykernel_13468\\3315288809.py:143: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  best_indices = np.argsort(np.array(scores.flatten()))[-n:][::-1]  # Sort and reverse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'Text',\n",
       "  'content': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the\\nyrder of the sequence, we must inject some information about the relative or absolute position of the\\nokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nottoms of the encoder and decoder stacks. The positional encodings have the same dimension dode|\\nis the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nearned and fixed [9].\\n',\n",
       "  'page': 5,\n",
       "  'bbox': [464.0125732421875,\n",
       "   973.0946655273438,\n",
       "   2113.4306640625,\n",
       "   1254.83251953125]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persistence test\n",
    "pipeline = DocumentAnalysis()\n",
    "pipeline.faiss_read()\n",
    "query=\"How does a Transformer use positional embeddings?\"\n",
    "pipeline.search_faiss(query, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model experimentation (Manual Annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current doc: 1706.03762.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a168507df11b49ed88996959ae1656e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2f8e16cabd477eb893dae67186e94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]c:\\Users\\lewis\\OneDrive\\Desktop\\Schoolwork\\FYP\\FYProject\\.venv\\Lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 15/15 [03:11<00:00, 12.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current doc: imagenet-classification.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:06<00:00, 14.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current doc: 2010.11929v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [05:15<00:00, 14.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# Experiment section: Q-A over 3 documents\n",
    "from document_analysis import DocumentAnalysis\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Helper functions\n",
    "# Formatting Q-A pairs with COCO annotations\n",
    "# Formula for IoU\n",
    "def calculate_iou(box1, box2):\n",
    "    box1 = np.array(box1, dtype=np.float32)\n",
    "    box2 = np.array(box2, dtype=np.float32)\n",
    "\n",
    "    # Compute intersection coordinates\n",
    "    inter_x_min = np.maximum(box1[0], box2[0])\n",
    "    inter_y_min = np.maximum(box1[1], box2[1])\n",
    "    inter_x_max = np.minimum(box1[2], box2[2])\n",
    "    inter_y_max = np.minimum(box1[3], box2[3])\n",
    "\n",
    "    # Compute intersection area\n",
    "    inter_width = np.maximum(0, inter_x_max - inter_x_min)\n",
    "    inter_height = np.maximum(0, inter_y_max - inter_y_min)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    # Compute area of both boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # Compute union area with numerical stability\n",
    "    union_area = np.maximum(box1_area + box2_area - inter_area, 1e-10)\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = inter_area / union_area\n",
    "    return np.clip(iou, 0.0, 1.0)\n",
    "\n",
    "# Convert (x, y, width, height) to (x1, y1, x2, y2)\n",
    "def coco_to_coordinates(bbox):\n",
    "    x, y, width, height = bbox # unwrap\n",
    "    return [x, y, x + width, y + height]\n",
    "\n",
    "\n",
    "# Filename init\n",
    "data_dir = \"../data/\"\n",
    "data_annotations_dir = \"../data/annotations/\"\n",
    "doc_list = [\"1706.03762.pdf\", \"imagenet-classification.pdf\", \"2010.11929v2.pdf\"]\n",
    "annotations_list = [\"annotations-attention\", \"annotations-imagenet\", \"annotations-vit\"]\n",
    "qa_pairs_list = [\"qa-pairs-attention\", \"qa-pairs-imagenet\", \"qa-pairs-vit\"]\n",
    "\n",
    "# Metrics\n",
    "# Total pages, questions and total regions fixed at 20*3 = 60\n",
    "correct_pages_top1 = 0\n",
    "correct_regions_top1 = 0\n",
    "correct_regions_iou_top1 = 0.0\n",
    "\n",
    "correct_pages_top5 = 0\n",
    "correct_regions_top5 = 0\n",
    "correct_regions_iou_top5 = 0.0\n",
    "\n",
    "for doc_name, annotations_name, qa_pairs_name in zip(doc_list, annotations_list, qa_pairs_list):\n",
    "    # if doc_name == \"1706.03762.pdf\": continue\n",
    "    print(f\"Current doc: {doc_name}\")\n",
    "    annotations = json.load(open(data_annotations_dir + annotations_name + '.json', 'r'))\n",
    "    qa_pairs = json.load(open(data_annotations_dir + qa_pairs_name + '.json', 'r'))\n",
    "\n",
    "    # New pipeline, read and process \n",
    "    pipeline = DocumentAnalysis(vector_dir = '../data/.vectorstore/')\n",
    "    doc = pipeline.read_from_path(data_dir + doc_name)\n",
    "    pipeline.process_document(doc)\n",
    "    pipeline.faiss_persist(subdir = doc_name + '/') # one-time, update schema\n",
    "    # pipeline.faiss_read(subdir = doc_name + '/') # debug only, uncomment above\n",
    "    continue # REMOVE AFTER STORED THEN READ ONLY\n",
    "\n",
    "    # Q-A assessment\n",
    "    for qa in qa_pairs:\n",
    "        qvalue, qpage, qbbox = qa['answer'].values()\n",
    "        qbbox = tuple(coco_to_coordinates(qbbox)) # Standardize to LayoutParser bbox system\n",
    "\n",
    "        answers = pipeline.search_faiss(qa['question'])\n",
    "        atext = [a['content'] for a in answers]\n",
    "        apages = [a['page']+1 for a in answers] # Add 1 to convert from index to numbering\n",
    "        abboxes = [a['bbox'] for a in answers]\n",
    "\n",
    "        # Verbose illustration\n",
    "        print(f'ground: {qpage}, {qbbox}, {qa['question']}')\n",
    "        for i in zip(apages, abboxes, atext):\n",
    "            print(i)\n",
    "\n",
    "        # Top-1 metric\n",
    "        # apages is ordered in decreasing order\n",
    "        if apages[0] == qpage:\n",
    "            correct_pages_top1 += 1\n",
    "            iou = calculate_iou(qbbox, abboxes[0])\n",
    "            print(f'iou_top1: {iou}')\n",
    "            if iou > 0.5: \n",
    "                correct_regions_top1 += 1\n",
    "                correct_regions_iou_top1 += iou\n",
    "        \n",
    "        # Top-5 metric\n",
    "        if qpage in apages:\n",
    "            print(qpage, apages)\n",
    "            correct_pages_top5 += 1\n",
    "            for apage, abbox in zip(apages, abboxes):\n",
    "                if apage == qpage:\n",
    "                    iou = calculate_iou(qbbox, abbox)\n",
    "                    print(f'iou_top5: {iou}')\n",
    "                    if iou > 0.5: \n",
    "                        correct_regions_top5 += 1\n",
    "                        correct_regions_iou_top5 += iou\n",
    "                        break # If correct found, don't include other chunks\n",
    "        print()\n",
    "    print(correct_pages_top1, correct_regions_top1, correct_pages_top5, correct_regions_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'correct_pages_top1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrect pages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcorrect_pages_top1\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrect regions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_regions_top1\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, IoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_regions_iou_top1\u001b[38;5;241m/\u001b[39mcorrect_regions_iou_top1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop-5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'correct_pages_top1' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Top-1\")\n",
    "print(f'Correct pages: {correct_pages_top1/60}')\n",
    "print(f'Correct regions: {correct_regions_top1/60}, IoU: {correct_regions_iou_top1/correct_regions_iou_top1}')\n",
    "print(\"Top-5\")\n",
    "print(f\"Correct pages: {correct_pages_top5/60}\")\n",
    "print(f'Correct regions: {correct_regions_top5/60}, IoU: {correct_regions_iou_top5/correct_regions_top5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Transformers are awesome\"\n",
    "e1 = pipeline.encode_text(test)\n",
    "e2 = pipeline.get_voyage_embeddings(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "faiss_index = faiss.IndexFlatL2(1024)\n",
    "print(faiss_index.is_trained)\n",
    "faiss_index.add(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
