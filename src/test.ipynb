{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "import pymupdf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPConfig, CLIPTokenizer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import voyageai\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# os.environ[\"VOYAGE_API_KEY\"] = \"pa-t-QdSeBOYxYQ83TObGLxkR4iqMZpYylSWOLBmthFUG7\"\n",
    "\n",
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified class for processing, analyzing and storing a document\n",
    "class DocumentAnalysis():\n",
    "    def __init__(self, embedding_model = \"openai/clip-vit-base-patch32\", cross_encoder_model = \"cross-encoder/ms-marco-MiniLM-L6-v2\"):\n",
    "         # Layout detection\n",
    "        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_R_50_FPN_3x/config', \n",
    "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n",
    "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"},\n",
    "                                 device=device)\n",
    "        self.ocr_agent = lp.TesseractAgent(languages='eng') \n",
    "\n",
    "        # Dual encoders for embeddings\n",
    "        self.clip_model = CLIPModel.from_pretrained(embedding_model, device_map=device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(embedding_model)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(embedding_model)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(self.tokenizer, chunk_size=77, chunk_overlap=5)\n",
    "\n",
    "        # Cross encoder for retrieval-reranking\n",
    "        self.cross_encoder_tokenizer = AutoTokenizer.from_pretrained(cross_encoder_model)\n",
    "        self.cross_encoder = AutoModelForSequenceClassification.from_pretrained(cross_encoder_model).to(device)       \n",
    "\n",
    "        # Vectorstore variables\n",
    "        self.dimension = 512  # CLIP's embedding size\n",
    "        self.faiss_index = faiss.IndexFlatL2(self.dimension) # FAISS Vector store\n",
    "        self.metadata_store = {}  # Store mapping of IDs and document page number to content\n",
    "        self.vector_dir = '../data/.vectorstore/' # Directory to write data to\n",
    "\n",
    "    # Read a PDF document using PyMuPDF\n",
    "    # Returns list of page images in cv2 format\n",
    "    def read_from_path(self, filepath):\n",
    "        doc = pymupdf.open(filepath)\n",
    "        return [self.pixmap_to_cv2(page.get_pixmap(dpi=300)) for page in doc]\n",
    "\n",
    "    # Convert PyMuPDF pixmap to cv2\n",
    "    def pixmap_to_cv2(self, pixmap):\n",
    "        bytes = np.frombuffer(pixmap.samples, dtype=np.uint8)\n",
    "        image = bytes.reshape(pixmap.height, pixmap.width, pixmap.n)\n",
    "        image = image[..., ::-1]\n",
    "        return image\n",
    "\n",
    "    # Takes in image object from read_from_path()\n",
    "    # Detects layout -> Processes ROI by label\n",
    "    def detect_layout(self, image):\n",
    "        layout = self.model.detect(image)\n",
    "\n",
    "        # Separate boxes by category\n",
    "        text_blocks = lp.Layout([b for b in layout if b.type=='Text'])\n",
    "        title_blocks = lp.Layout([b for b in layout if b.type=='Title'])\n",
    "        list_blocks = lp.Layout([b for b in layout if b.type=='List'])\n",
    "        table_blocks = lp.Layout([b for b in layout if b.type=='Table'])\n",
    "        figure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])\n",
    "\n",
    "        # Processing text blocks\n",
    "        # Sourced from LayoutParser's Deep Layout Analysis example\n",
    "        # Eliminate text blocks nested in images/figures\n",
    "        text_blocks = lp.Layout([b for b in text_blocks \\\n",
    "                        if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "        # Sort boxes\n",
    "        h, w = image.shape[:2]\n",
    "        left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
    "        left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
    "        left_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "        # The b.coordinates[1] corresponds to the y coordinate of the region\n",
    "        # sort based on that can simulate the top-to-bottom reading order \n",
    "        right_blocks = lp.Layout([b for b in text_blocks if b not in left_blocks])\n",
    "        right_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "\n",
    "        # And finally combine the two lists and add the index\n",
    "        text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
    "\n",
    "        # Perform OCR to extract text\n",
    "        for block in text_blocks + title_blocks + list_blocks + table_blocks + figure_blocks:\n",
    "            # Add padding in each image segment to improve robustness\n",
    "            text = self._ocr_on_block(image, block)\n",
    "            block.set(text=text, inplace=True) # Assign parsed text to block element\n",
    "            \n",
    "        # Return all blocks on the page as a list\n",
    "        return text_blocks + title_blocks + list_blocks + table_blocks + figure_blocks\n",
    "\n",
    "    # Function to crop an image given block's bbox and additional padding\n",
    "    def _crop_image(self, image, block, padding=10):\n",
    "        return (block.pad(left=padding, right=padding, top=padding, bottom=padding).crop_image(image))\n",
    "\n",
    "    # Perform OCR to extract text given image and block (for text, tables and lists)\n",
    "    def _ocr_on_block(self, image, block):\n",
    "        # Add padding in each image segment to improve robustness\n",
    "        segment_image = (block.pad(left=5, right=5, top=5, bottom=5).crop_image(image))\n",
    "        return self.ocr_agent.detect(segment_image)\n",
    "\n",
    "    # Vectorstore functions\n",
    "    # Function to chunk text to CLIP max length\n",
    "    def chunk_text(self, text):\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        return chunks\n",
    "\n",
    "    # Function to encode text\n",
    "    def encode_text(self, text):\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_text_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to encode image\n",
    "    def encode_image(self, image):\n",
    "        inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_image_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to encode both image and text simultaneously\n",
    "    def encode_multimodal(self, image, text=None):\n",
    "        # If no text detected, format to empty list\n",
    "        if text is None or len(text)==0: text=[]\n",
    "        inputs = self.clip_processor(images=image, text=[text], return_tensors='pt')\n",
    "        # Get image embeddings\n",
    "        inputs_image = {'pixel_values': inputs['pixel_values']}\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_image_features(**inputs_image).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "        \n",
    "        \n",
    "    # Function to add item to FAISS\n",
    "    # Specify content, type, page and bounding box from blocks\n",
    "    def add_to_faiss(self, embedding, content, content_type, page_idx, bbox):\n",
    "        idx = len(self.metadata_store)  # Assign unique index\n",
    "        self.faiss_index.add(embedding)\n",
    "        self.metadata_store[idx] = {\"type\": content_type, \"content\": content, \"page\": page_idx, \"bbox\": bbox}\n",
    "    \n",
    "    # Perform retrieval and reranking\n",
    "    def search_faiss(self, query, k=15, n=5):\n",
    "        query_embedding = self.encode_text(query)\n",
    "        _, indices = self.faiss_index.search(query_embedding, k)\n",
    "        indices = [int(i) for i in indices[0]]\n",
    "        \n",
    "        # Cross encoder reranking on text modality\n",
    "        answers = [self.metadata_store[idx] for idx in indices if self.metadata_store[idx]['type']!='Figure']\n",
    "        answer_texts = [a['content'] for a in answers]\n",
    "        queries = [query for i in range(len(answers))] # Repeat for tokenizer input\n",
    "        features = self.cross_encoder_tokenizer(queries, answer_texts,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad(): # Rerank\n",
    "            scores = self.cross_encoder(**features).logits\n",
    "\n",
    "        # Get indices of the top n scores)\n",
    "        best_indices = np.argsort(np.array(scores.flatten()))[-n:][::-1]  # Sort and reverse\n",
    "\n",
    "        # Retrieve responses using the indices\n",
    "        best_answers = [answers[i] for i in best_indices]\n",
    "        return best_answers\n",
    "\n",
    "\n",
    "    # Writes the vectorstore and metadata into a given path\n",
    "    def faiss_persist(self):\n",
    "        faiss.write_index(self.faiss_index, self.vector_dir+\"faiss.index\")\n",
    "        json.dump(self.metadata_store, open(self.vector_dir+\"metadata.json\", 'w'))\n",
    "    \n",
    "    # Read from existing vector stores\n",
    "    def faiss_read(self):\n",
    "        self.faiss_index = faiss.read_index(self.vector_dir+\"faiss.index\")\n",
    "        self.metadata_store = json.load(open(self.vector_dir+\"metadata.json\", 'r'), object_hook=self._convert_keys)\n",
    "    \n",
    "    # Convert keys from string to int when deserializing\n",
    "    def _convert_keys(self, d):\n",
    "        return {int(k) if k.isdigit() else k: v for k, v in d.items()}\n",
    "\n",
    "    # Function to process all pages of a document given all the functions above\n",
    "    # Returns nothing, processes and ingests document into the object's metadata store\n",
    "    def process_document(self, doc):\n",
    "        for page_idx, page in enumerate(tqdm(doc)):\n",
    "            blocks = self.detect_layout(page)\n",
    "\n",
    "            # Processing for each block to be vectorized\n",
    "            for b in blocks:\n",
    "                if b.type == \"Text\":\n",
    "                    # Chunk text and create new blocks, and process for each block\n",
    "                    # Returns list even if unchanged\n",
    "                    chunks = self.chunk_text(b.text)\n",
    "                    for chunk in chunks:\n",
    "                        # Encode as text and add to FAISS\n",
    "                        # Embeddings use the chunks, but metadata contains original text for completion\n",
    "                        text_embs = self.encode_text(chunk)\n",
    "                        self.add_to_faiss(\n",
    "                            embedding=text_embs, \n",
    "                            content=b.text, \n",
    "                            content_type=b.type, \n",
    "                            page_idx=page_idx, \n",
    "                            bbox=b.block.coordinates\n",
    "                        )\n",
    "                else:\n",
    "                    # Multimodal for images and layout\n",
    "                    # Crop and get image embeddings\n",
    "                    segmented_image = self._crop_image(page, b, padding=20)\n",
    "                    multimodal_embs = self.encode_multimodal(segmented_image, b.text)\n",
    "                    self.add_to_faiss(\n",
    "                        embedding=multimodal_embs, \n",
    "                        content=b.text, \n",
    "                        content_type=b.type, \n",
    "                        page_idx=page_idx, \n",
    "                        bbox=b.block.coordinates\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample run for 1 document\n",
    "# For debug, run pipeline.faiss_read() in cell below to prevent rereading doc\n",
    "pipeline = DocumentAnalysis()\n",
    "doc_path = \"../data/1706.03762.pdf\"\n",
    "doc = pipeline.read_from_path(doc_path)\n",
    "pipeline.process_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'Text', 'content': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the\\nyrder of the sequence, we must inject some information about the relative or absolute position of the\\nokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nottoms of the encoder and decoder stacks. The positional encodings have the same dimension dode|\\nis the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nearned and fixed [9].\\n', 'page': 5, 'bbox': [464.0125732421875, 973.0946655273438, 2113.4306640625, 1254.83251953125]}, {'type': 'Text', 'content': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the\\nyrder of the sequence, we must inject some information about the relative or absolute position of the\\nokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nottoms of the encoder and decoder stacks. The positional encodings have the same dimension dode|\\nis the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nearned and fixed [9].\\n', 'page': 5, 'bbox': [464.0125732421875, 973.0946655273438, 2113.4306640625, 1254.83251953125]}, {'type': 'Text', 'content': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\n', 'page': 1, 'bbox': [450.40130615234375, 1614.4937744140625, 2103.35205078125, 2032.34375]}, {'type': 'Text', 'content': 'The Transformer uses multi-head attention in three different ways:\\n', 'page': 4, 'bbox': [457.04034423828125, 1247.1748046875, 1558.388671875, 1290.0250244140625]}, {'type': 'Text', 'content': 'To the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n', 'page': 1, 'bbox': [455.5078430175781, 2412.51611328125, 2113.541015625, 2598.93115234375]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lewis\\AppData\\Local\\Temp\\ipykernel_13468\\3315288809.py:143: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  best_indices = np.argsort(np.array(scores.flatten()))[-n:][::-1]  # Sort and reverse\n"
     ]
    }
   ],
   "source": [
    "# Vectordb search test and persist test\n",
    "pipeline.faiss_persist()\n",
    "query=\"How does a Transformer use positional encoding?\"\n",
    "answer = pipeline.search_faiss(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lewis\\AppData\\Local\\Temp\\ipykernel_13468\\3315288809.py:143: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  best_indices = np.argsort(np.array(scores.flatten()))[-n:][::-1]  # Sort and reverse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'Text',\n",
       "  'content': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the\\nyrder of the sequence, we must inject some information about the relative or absolute position of the\\nokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nottoms of the encoder and decoder stacks. The positional encodings have the same dimension dode|\\nis the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nearned and fixed [9].\\n',\n",
       "  'page': 5,\n",
       "  'bbox': [464.0125732421875,\n",
       "   973.0946655273438,\n",
       "   2113.4306640625,\n",
       "   1254.83251953125]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persistence test\n",
    "pipeline = DocumentAnalysis()\n",
    "pipeline.faiss_read()\n",
    "query=\"How does a Transformer use positional embeddings?\"\n",
    "pipeline.search_faiss(query, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model experimentation (Manual Annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current doc: 1706.03762.pdf\n",
      "\n",
      "Question: What is the main contribution of the Transformer model?\n",
      "('“Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n', 1, [303.6634521484375, 1659.4560546875, 1411.9486083984375, 1912.21142578125])\n",
      "('To the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n', 2, [304.999755859375, 1607.9449462890625, 1408.7899169921875, 1732.0218505859375])\n",
      "('In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n', 2, [297.3193359375, 834.3128051757812, 1398.734619140625, 959.2935180664062])\n",
      "('In this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\n', 10, [299.38653564453125, 1081.5145263671875, 1403.79833984375, 1169.124267578125])\n",
      "('The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n', 1, [399.3802185058594, 1135.9677734375, 1302.9542236328125, 1609.6142578125])\n",
      "2 [1, 2, 2, 10, 1]\n",
      "iou_top5: 0.9677773714065552\n",
      "\n",
      "Question: What tasks has the self-attention mechanism demonstrated success in?\n",
      "('Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\n', 2, [304.2303771972656, 1363.4677734375, 1400.4775390625, 1489.056884765625])\n",
      "('Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\n', 2, [300.27862548828125, 700.5811767578125, 1406.7139892578125, 821.861083984375])\n",
      "('As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n', 7, [298.7570495605469, 655.6446533203125, 1395.0526123046875, 781.0455322265625])\n",
      "('We are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\n', 10, [297.7655029296875, 1325.2177734375, 1417.8319091796875, 1447.61669921875])\n",
      "('End-to-end memory networks are based on a recurrent attention mechanism instead of sequence\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\n', 2, [300.18017578125, 1505.3875732421875, 1393.52734375, 1595.6168212890625])\n",
      "iou_top1: 0.9543092846870422\n",
      "2 [2, 2, 7, 10, 2]\n",
      "iou_top5: 0.9543092846870422\n",
      "\n",
      "Question: How are encoders used in the Transformer architecture?\n",
      "('In this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\n', 10, [299.38653564453125, 1081.5145263671875, 1403.79833984375, 1169.124267578125])\n",
      "('The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n', 3, [303.991455078125, 1209.4967041015625, 1403.126220703125, 1304.418701171875])\n",
      "('The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n', 1, [399.3802185058594, 1135.9677734375, 1302.9542236328125, 1609.6142578125])\n",
      "('In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\n', 5, [301.32379150390625, 1436.1031494140625, 1399.3084716796875, 1530.4207763671875])\n",
      "('Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer() is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmode = 512.\\n', 3, [297.13177490234375, 1394.7139892578125, 1403.3853759765625, 1611.6348876953125])\n",
      "3 [10, 3, 1, 5, 3]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.9767695665359497\n",
      "\n",
      "Question: How many layers does the Transformer decoder have?\n",
      "('Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than 7.\\n', 3, [300.8603210449219, 1640.613525390625, 1403.1871337890625, 1862.01025390625])\n",
      "('In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\n', 5, [301.32379150390625, 1436.1031494140625, 1399.3084716796875, 1530.4207763671875])\n",
      "('The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n', 3, [303.991455078125, 1209.4967041015625, 1403.126220703125, 1304.418701171875])\n",
      "('We trained a 4-layer transformer with dodet = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nising the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\n', 9, [309.6208801269531, 1749.982177734375, 1402.7545166015625, 1903.104736328125])\n",
      "('In this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\n', 10, [299.38653564453125, 1081.5145263671875, 1403.79833984375, 1169.124267578125])\n",
      "iou_top1: 0.9473170042037964\n",
      "3 [3, 5, 3, 9, 10]\n",
      "iou_top5: 0.9473170042037964\n",
      "\n",
      "Question: How is scaled dot-product attention computed in the Transformer?\n",
      "('We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension d;,, and values of dimension d,,. We compute the dot products of the\\n\\nquery with all keys, divide each by Wd; and apply a softmax function to obtain the weights on the\\nvalues.\\n', 4, [296.73095703125, 1022.8419189453125, 1401.71484375, 1152.03564453125])\n",
      "('Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of sever\\nattention layers running in parallel.\\n', 4, [301.313232421875, 758.9078369140625, 1374.5496826171875, 821.99462890625])\n",
      "('The two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nPlicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof TE Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\n', 4, [302.246826171875, 1382.530517578125, 1402.848388671875, 1576.0006103515625])\n",
      "('3.2.1 Scaled Dot-Product Attention\\n', 4, [300.7055969238281, 972.1287841796875, 738.6445922851562, 1007.3341064453125])\n",
      "('While for small values of dj, the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of d;, [3]. We suspect that for large values of\\nd,,, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by Tir\\n', 4, [295.760009765625, 1589.76806640625, 1402.8258056640625, 1718.430908203125])\n",
      "iou_top1: 0.9120153784751892\n",
      "4 [4, 4, 4, 4, 4]\n",
      "iou_top5: 0.9120153784751892\n",
      "\n",
      "Question: What is the purpose of the scaling factor in scaled dot-product attention?\n",
      "('We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension d;,, and values of dimension d,,. We compute the dot products of the\\n\\nquery with all keys, divide each by Wd; and apply a softmax function to obtain the weights on the\\nvalues.\\n', 4, [296.73095703125, 1022.8419189453125, 1401.71484375, 1152.03564453125])\n",
      "('The two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nPlicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof TE Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\n', 4, [302.246826171875, 1382.530517578125, 1402.848388671875, 1576.0006103515625])\n",
      "('While for small values of dj, the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of d;, [3]. We suspect that for large values of\\nd,,, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by Tir\\n', 4, [295.760009765625, 1589.76806640625, 1402.8258056640625, 1718.430908203125])\n",
      "('3.2.1 Scaled Dot-Product Attention\\n', 4, [300.7055969238281, 972.1287841796875, 738.6445922851562, 1007.3341064453125])\n",
      "('Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of sever\\nattention layers running in parallel.\\n', 4, [301.313232421875, 758.9078369140625, 1374.5496826171875, 821.99462890625])\n",
      "iou_top1: 0.0\n",
      "4 [4, 4, 4, 4, 4]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.9741644859313965\n",
      "\n",
      "Question: Why does leftward information flow need to be prevented in the decoder layer?\n",
      "('¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\n¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\n¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to —oo) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n', 5, [361.40313720703125, 891.035888671875, 1398.248291015625, 1346.5072021484375])\n",
      "('Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than 7.\\n', 3, [300.8603210449219, 1640.613525390625, 1403.1871337890625, 1862.01025390625])\n",
      "('Decoder:\\n', 3, [301.0030822753906, 1642.9586181640625, 407.799072265625, 1671.8299560546875])\n",
      "('In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\n', 5, [301.32379150390625, 1436.1031494140625, 1399.3084716796875, 1530.4207763671875])\n",
      "('The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n', 3, [303.991455078125, 1209.4967041015625, 1403.126220703125, 1304.418701171875])\n",
      "iou_top1: 0.9582011103630066\n",
      "5 [5, 3, 3, 5, 3]\n",
      "iou_top5: 0.9582011103630066\n",
      "\n",
      "Question: How many parallel attention layers (heads) are used in the Transformer?\n",
      "('In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndj, = dy = dmodei/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n', 5, [301.76739501953125, 652.7040405273438, 1396.226318359375, 745.0515747070312])\n",
      "('The Transformer uses multi-head attention in three different ways:\\n', 5, [303.81134033203125, 831.7400512695312, 1038.3359375, 860.3290405273438])\n",
      "('In this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\n', 10, [299.38653564453125, 1081.5145263671875, 1403.79833984375, 1169.124267578125])\n",
      "('Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of sever\\nattention layers running in parallel.\\n', 4, [301.313232421875, 758.9078369140625, 1374.5496826171875, 821.99462890625])\n",
      "('The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n', 3, [303.991455078125, 1209.4967041015625, 1403.126220703125, 1304.418701171875])\n",
      "iou_top1: 0.9245194792747498\n",
      "5 [5, 5, 10, 4, 3]\n",
      "iou_top5: 0.9245194792747498\n",
      "\n",
      "Question: How does the Transformer convert input tokens to embeddings?\n",
      "('Similarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodei. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \\\\/dmodel.\\n', 5, [304.33770751953125, 1855.93701171875, 1398.342041015625, 2011.1358642578125])\n",
      "('To the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n', 2, [304.999755859375, 1607.9449462890625, 1408.7899169921875, 1732.0218505859375])\n",
      "('In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n', 2, [297.3193359375, 834.3128051757812, 1398.734619140625, 959.2935180664062])\n",
      "('The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n', 3, [303.991455078125, 1209.4967041015625, 1403.126220703125, 1304.418701171875])\n",
      "('The Transformer uses multi-head attention in three different ways:\\n', 5, [303.81134033203125, 831.7400512695312, 1038.3359375, 860.3290405273438])\n",
      "iou_top1: 0.9716058373451233\n",
      "5 [5, 2, 2, 3, 5]\n",
      "iou_top5: 0.9716058373451233\n",
      "\n",
      "Question: How does the computational complexity of a self-attention layer compare to recurrent and convolutional layers?\n",
      "('As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n', 6, [301.6290588378906, 1914.496337890625, 1403.69970703125, 2009.470703125])\n",
      "('In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(21,...,@n) to another sequence of equal length (z1,...,2n), with 2;, 2; € IR, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\n', 6, [293.12005615234375, 1442.6717529296875, 1408.777099609375, 1597.699951171875])\n",
      "('A single convolutional layer with kernel width k < n does not connect all pairs of input and outpu\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels\\nor O(log,(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive thar\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k -n-d+n-d?). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer\\nthe approach we take in our model.\\n', 7, [305.9185485839844, 401.5473327636719, 1391.650634765625, 647.6824340820312])\n",
      "('Layer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\n\\nSelf-Attention (n? - d) O(1) O(1)\\n\\nRecurrent (n+ d?) O(n) O(n)\\n\\nConvolutional O(k-n-d?) O(1) O(logx(n))\\n\\nSelf-Attention (restricted) O(r-n-d) ol) O(n/r)\\n\\n', 6, [353.54803466796875, 319.8682861328125, 1367.7501220703125, 523.511962890625])\n",
      "('Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operation:\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kerne\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\n', 6, [306.27764892578125, 194.1731414794922, 1388.6036376953125, 288.7279968261719])\n",
      "iou_top1: 0.9492151141166687\n",
      "6 [6, 6, 7, 6, 6]\n",
      "iou_top5: 0.9492151141166687\n",
      "\n",
      "Question: Why is sinusoidal positional encoding preferred over learned positional encoding?\n",
      "('We also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n', 6, [307.86181640625, 1213.0372314453125, 1394.280029296875, 1334.3173828125])\n",
      "('where pos is the position and 7 is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 27 to 10000 - 27. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+, can be represented as a linear function of\\nPE,\\n\\n(pos\\n', 6, [299.458984375, 1046.0330810546875, 1405.3994140625, 1202.9393310546875])\n",
      "('In Table 3 rows (B), we observe that reducing the attention key size dj, hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n', 9, [308.47979736328125, 1328.4014892578125, 1401.000732421875, 1514.7802734375])\n",
      "('Since our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\n‘okens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\ndottoms of the encoder and decoder stacks. The positional encodings have the same dimension dyodet\\nis the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\n', 6, [309.361572265625, 648.6790161132812, 1408.7398681640625, 836.1653442382812])\n",
      "('3.5 Positional Encoding\\n', 6, [300.6069641113281, 590.979736328125, 603.0147094726562, 625.7772827148438])\n",
      "iou_top1: 0.9680817723274231\n",
      "6 [6, 6, 9, 6, 6]\n",
      "iou_top5: 0.9680817723274231\n",
      "\n",
      "Question: Why is self-attention preferred over convolutional or recurrent layers?\n",
      "('As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n', 6, [301.6290588378906, 1914.496337890625, 1403.69970703125, 2009.470703125])\n",
      "('In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(21,...,@n) to another sequence of equal length (z1,...,2n), with 2;, 2; € IR, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\n', 6, [293.12005615234375, 1442.6717529296875, 1408.777099609375, 1597.699951171875])\n",
      "('A single convolutional layer with kernel width k < n does not connect all pairs of input and outpu\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels\\nor O(log,(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive thar\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k -n-d+n-d?). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer\\nthe approach we take in our model.\\n', 7, [305.9185485839844, 401.5473327636719, 1391.650634765625, 647.6824340820312])\n",
      "('In this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\n', 10, [299.38653564453125, 1081.5145263671875, 1403.79833984375, 1169.124267578125])\n",
      "('Layer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\n\\nSelf-Attention (n? - d) O(1) O(1)\\n\\nRecurrent (n+ d?) O(n) O(n)\\n\\nConvolutional O(k-n-d?) O(1) O(logx(n))\\n\\nSelf-Attention (restricted) O(r-n-d) ol) O(n/r)\\n\\n', 6, [353.54803466796875, 319.8682861328125, 1367.7501220703125, 523.511962890625])\n",
      "iou_top1: 0.9492151141166687\n",
      "6 [6, 6, 7, 10, 6]\n",
      "iou_top5: 0.9492151141166687\n",
      "\n",
      "Question: Can self-attention make the model's performance more interpretable?\n",
      "('As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n', 7, [298.7570495605469, 655.6446533203125, 1395.0526123046875, 781.0455322265625])\n",
      "('length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\n', 7, [300.9000244140625, 203.53863525390625, 1406.650634765625, 391.2835388183594])\n",
      "('To the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n', 2, [304.999755859375, 1607.9449462890625, 1408.7899169921875, 1732.0218505859375])\n",
      "('In this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\n', 10, [299.38653564453125, 1081.5145263671875, 1403.79833984375, 1169.124267578125])\n",
      "('Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\n', 2, [304.2303771972656, 1363.4677734375, 1400.4775390625, 1489.056884765625])\n",
      "iou_top1: 0.9230312705039978\n",
      "7 [7, 7, 2, 10, 2]\n",
      "iou_top5: 0.9230312705039978\n",
      "\n",
      "Question: What optimizer was used to train the Transformer model?\n",
      "('In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkele\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n', 10, [290.14117431640625, 910.4927978515625, 1382.4927978515625, 971.720947265625])\n",
      "('On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Papop = 0.1, instead of 0.3.\\n', 8, [302.77508544921875, 1356.5302734375, 1424.645751953125, 1480.1328125])\n",
      "('For translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\n', 10, [300.47711181640625, 1187.361328125, 1402.181640625, 1308.7276611328125])\n",
      "('The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n', 1, [399.3802185058594, 1135.9677734375, 1302.9542236328125, 1609.6142578125])\n",
      "('In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n', 2, [297.3193359375, 834.3128051757812, 1398.734619140625, 959.2935180664062])\n",
      "\n",
      "Question: How were sentences encoded in the English-German dataset?\n",
      "('We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n', 7, [298.85205078125, 1032.166259765625, 1408.678955078125, 1250.42919921875])\n",
      "('We performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n', 9, [295.2104187011719, 1914.3282470703125, 1410.193359375, 2007.7552490234375])\n",
      "('The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n', 1, [399.3802185058594, 1135.9677734375, 1302.9542236328125, 1609.6142578125])\n",
      "('For translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\n', 10, [300.47711181640625, 1187.361328125, 1402.181640625, 1308.7276611328125])\n",
      "('To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n', 8, [291.46759033203125, 1883.765869140625, 1421.51708984375, 1947.186767578125])\n",
      "iou_top1: 0.9519891142845154\n",
      "7 [7, 9, 1, 10, 8]\n",
      "iou_top5: 0.9519891142845154\n",
      "\n",
      "Question: How is residual dropout used in the Transformer architecture?\n",
      "('Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nParop = 0.1.\\n', 8, [298.2413024902344, 756.94287109375, 1394.880615234375, 880.8865966796875])\n",
      "('On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Papop = 0.1, instead of 0.3.\\n', 8, [302.77508544921875, 1356.5302734375, 1424.645751953125, 1480.1328125])\n",
      "('In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n', 2, [297.3193359375, 834.3128051757812, 1398.734619140625, 959.2935180664062])\n",
      "('In this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\n', 10, [299.38653564453125, 1081.5145263671875, 1403.79833984375, 1169.124267578125])\n",
      "('We performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n', 9, [295.2104187011719, 1914.3282470703125, 1410.193359375, 2007.7552490234375])\n",
      "iou_top1: 0.9639056921005249\n",
      "8 [8, 8, 2, 10, 9]\n",
      "iou_top5: 0.9639056921005249\n",
      "\n",
      "Question: How long did the base model and big model train for?\n",
      "('We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\n\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n', 7, [301.3261413574219, 1348.343994140625, 1402.7178955078125, 1505.886962890625])\n",
      "('On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\n', 8, [299.0964050292969, 1157.1748046875, 1406.7354736328125, 1342.816162109375])\n",
      "('‘or the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nvere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nised beam search with a beam size of 4 and length penalty a = 0.6 [38]. These hyperparameters\\nvere chosen after experimentation on the development set. We set the maximum output length during\\nnference to input length + 50, but terminate early when possible [38].\\n', 8, [312.90301513671875, 1491.037841796875, 1404.8704833984375, 1653.2093505859375])\n",
      "('On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Papop = 0.1, instead of 0.3.\\n', 8, [302.77508544921875, 1356.5302734375, 1424.645751953125, 1480.1328125])\n",
      "('train | PPL BLEU params\\nN  dnodet deg h di, d, Parop Els steps | (dev) (dev) «108\\n\\nbase | 6 512 2048 8 64 64 0.1 O01 100K | 4.92 25.8 65\\n1 512 512 5.29 24.9\\n(A) 4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32. 16 ~— «16 5.01 25.4\\n\\n(B) 16 5.16 25.1 58\\n\\n32 5.01 25.4 60\\n\\n2 6.11 23.7 36\\n\\n4 5.19 25.3 50\\n\\n8 488 25.5 80\\n\\n(C) 256 32-32 5.75 24.5 28\\n\\n1024 128 128 466 26.0 168\\n\\n1024 5.12 254 53\\n\\n4096 475 26.2 90\\n0.0 5.77 24.6\\n0.2 495 25.5\\n©) 0.0 467 253\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\n\\nbig | 6 1024 4096 16 0.3 300K | 4.33 26.4 213\\n\\n', 9, [291.0591735839844, 339.8785400390625, 1391.3133544921875, 1087.0614013671875])\n",
      "iou_top1: 0.9758082032203674\n",
      "7 [7, 8, 8, 8, 9]\n",
      "iou_top5: 0.9758082032203674\n",
      "\n",
      "Question: What BLEU score did the Transformer (big) model achieve on the English-to-French and English-to-German translation tasks respectively?\n",
      "('On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Papop = 0.1, instead of 0.3.\\n', 8, [302.77508544921875, 1356.5302734375, 1424.645751953125, 1480.1328125])\n",
      "('On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\n', 8, [299.0964050292969, 1157.1748046875, 1406.7354736328125, 1342.816162109375])\n",
      "('The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n', 1, [399.3802185058594, 1135.9677734375, 1302.9542236328125, 1609.6142578125])\n",
      "('Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\n', 8, [306.9272766113281, 195.99560546875, 1422.9853515625, 258.3423156738281])\n",
      "('For translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\n', 10, [300.47711181640625, 1187.361328125, 1402.181640625, 1308.7276611328125])\n",
      "iou_top1: 0.0\n",
      "8 [8, 8, 1, 8, 10]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.0\n",
      "\n",
      "Question: What other task was performed on the Transformer to test it's generalizability?\n",
      "('To evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\n', 9, [300.6889343261719, 1611.6556396484375, 1399.0972900390625, 1737.7733154296875])\n",
      "('For translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\n', 10, [300.47711181640625, 1187.361328125, 1402.181640625, 1308.7276611328125])\n",
      "('We are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\n', 10, [297.7655029296875, 1325.2177734375, 1417.8319091796875, 1447.61669921875])\n",
      "('The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n', 1, [399.3802185058594, 1135.9677734375, 1302.9542236328125, 1609.6142578125])\n",
      "('On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\n', 8, [299.0964050292969, 1157.1748046875, 1406.7354736328125, 1342.816162109375])\n",
      "iou_top1: 0.9526718854904175\n",
      "9 [9, 10, 10, 1, 8]\n",
      "iou_top5: 0.9526718854904175\n",
      "\n",
      "Question: What future improvements and research did the authors suggest?\n",
      "('We are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\n', 10, [297.7655029296875, 1325.2177734375, 1417.8319091796875, 1447.61669921875])\n",
      "('“Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n', 1, [303.6634521484375, 1659.4560546875, 1411.9486083984375, 1912.21142578125])\n",
      "('length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\n', 7, [300.9000244140625, 203.53863525390625, 1406.650634765625, 391.2835388183594])\n",
      "('Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates h;, as a function of the previous hidden state h,_; and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\n', 2, [301.64849853515625, 439.56610107421875, 1404.670166015625, 682.9530639648438])\n",
      "('30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXi\\npreprint arXiv: 1608.05859, 2016.\\n', 12, [316.8957214355469, 775.132080078125, 1386.96337890625, 834.546875])\n",
      "iou_top1: 0.9348692297935486\n",
      "10 [10, 1, 7, 2, 12]\n",
      "iou_top5: 0.9348692297935486\n",
      "Top-1\n",
      "Correct pages: 0.85\n",
      "Correct regions: 0.75, IoU: 0.9491169452667236\n",
      "Top-5\n",
      "Correct pages: 0.95\n",
      "Correct regions: 0.9, IoU: 0.9530814290046692\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current doc: imagenet-classification.pdf\n",
      "\n",
      "Question: How many parameters does the convolutional neural network have?\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "('Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC\\nmake each training example impose 10 bits of constraint on the mapping from image to label, this\\nturns out to be insufficient to learn so many parameters without considerable overfitting. Below, we\\ndescribe the two primary ways in which we combat overfitting.\\n', 5, [307.04473876953125, 1137.6920166015625, 1399.484375, 1271.07958984375])\n",
      "('The specific contributions of this paper are as follows: we trained one of the largest convolutional\\nneural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012\\ncompetitions [2] and achieved by far the best results ever reported on these datasets. We wrote a\\nhighly-optimized GPU implementation of 2D convolution and all the other operations inherent in\\ntraining convolutional neural networks, which we make available publicly!. Our network contains\\na number of new and unusual features which improve its performance and reduce its training time,\\nwhich are detailed in Section 3. The size of our network made overfitting a significant problem, even\\nwith 1.2 million labeled training examples, so we used several effective techniques for preventing\\noverfitting, which are described in Section 4. Our final network contains five convolutional and\\nthree fully-connected layers, and this depth seems to be important: we found that removing any\\nconvolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in\\ninferior performance.\\n', 2, [306.3596496582031, 395.00555419921875, 1406.600830078125, 774.2710571289062])\n",
      "('fo learn about thousands of objects from millions of images, we need a model with a large learning\\n‘apacity. However, the immense complexity of the object recognition task means that this prob-\\nem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots\\n»f prior knowledge to compensate for all the data we don’t have. Convolutional neural networks\\nCNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be con-\\nrolled by varying their depth and breadth, and they also make strong and mostly correct assumptions\\nibout the nature of images (namely, stationarity of statistics and locality of pixel dependencies).\\nThus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have\\nnuch fewer connections and parameters and so they are easier to train, while their theoretically-bes!\\nyerformance is likely to be only slightly worse.\\n', 1, [312.1918640136719, 1727.4849853515625, 1391.3443603515625, 2036.3809814453125])\n",
      "('neurons in a kernel map). The second convolutional layer takes as input the (response-normalized\\nand pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 x 5 x 48.\\nThe third, fourth, and fifth convolutional layers are connected to one another without any intervening\\npooling or normalization layers. The third convolutional layer has 384 kernels of size 3 x 3 x\\n256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth\\nconvolutional layer has 384 kernels of size 3 x 3 x 192, and the fifth convolutional layer has 256\\nkernels of size 3 x 3 x 192. The fully-connected layers have 4096 neurons each.\\n', 5, [299.0901794433594, 803.8162841796875, 1401.75732421875, 1027.02587890625])\n",
      "iou_top1: 0.9648476243019104\n",
      "1 [1, 5, 2, 1, 5]\n",
      "iou_top5: 0.9648476243019104\n",
      "\n",
      "Question: Where is Dropout used in the architecture?\n",
      "('We use dropout in the first two fully-connected layers of Figure 2. Without dropout, our network ex-\\nhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge.\\n', 6, [285.4482421875, 1086.260009765625, 1429.4007568359375, 1148.740234375])\n",
      "('Combining the predictions of many different models is a very successful way to reduce test errors\\n[1, 3], but it appears to be too expensive for big neural networks that already take several days\\nto train. There is, however, a very efficient version of model combination that only costs about a\\nfactor of two during training. The recently-introduced technique, called “dropout” [10], consists\\nof setting to zero the output of each hidden neuron with probability 0.5. The neurons which are\\n“dropped out” in this way do not contribute to the forward pass and do not participate in back-\\npropagation. So every time an input is presented, the neural network samples a different architecture,\\nbut all these architectures share weights. This technique reduces complex co-adaptations of neurons,\\nsince a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\\nlearn more robust features that are useful in conjunction with many different random subsets of the\\nother neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a\\nreasonable approximation to taking the geometric mean of the predictive distributions produced by\\nthe exponentially-many dropout networks.\\n', 6, [301.25738525390625, 667.37939453125, 1395.9627685546875, 1076.7410888671875])\n",
      "('4.2 Dropout\\n', 6, [295.8933410644531, 617.853271484375, 463.1990966796875, 649.7906494140625])\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "iou_top1: 0.8626085519790649\n",
      "6 [6, 6, 6, 1, 1]\n",
      "iou_top5: 0.8626085519790649\n",
      "\n",
      "Question: What optimization algorithm was used to train the CNN, and what were its key hyperparameters?\n",
      "('Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture,\\nthey have still been prohibitively expensive to apply in large scale to high-resolution images. Luck-\\nily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful\\nenough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet\\ncontain enough labeled examples to train such models without severe overfitting.\\n', 2, [305.1011962890625, 230.115234375, 1399.6624755859375, 388.4068908691406])\n",
      "('The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Ciresan\\net al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1\\nand top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many\\nkernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time\\nto train than the one-GPU net’.\\n', 3, [300.44482421875, 1653.19921875, 1401.6768798828125, 1802.122802734375])\n",
      "('We are not the first to consider alternatives to tradi-\\ntional neuron models in CNNs. For example, Jarrett\\netal. [11] claim that the nonlinearity f(x) = |tanh(z)|\\nworks particularly well with their type of contrast nor-\\nmalization followed by local average pooling on the\\nCaltech-101 dataset. However, on this dataset the pri-\\nmary concern is preventing overfitting, so the effect\\nthey are observing is different from the accelerated\\nability to fit the training set which we report when us-\\ning ReLUs. Faster learning has a great influence on the\\nperformance of large models trained on large datasets.\\n', 3, [299.12567138671875, 833.2002563476562, 902.7128295898438, 1173.1689453125])\n",
      "('yf five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth con-\\nolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release\\n15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of\\n6.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 re-\\nease with the aforementioned five CNNs gives an error rate of 15.3%. The second-best con-\\nest entry achieved an error rate of 26.2% with an approach that averages the predictions of sev-\\nral classifiers trained on FVs computed from different types of densely-sampled features [7].\\n', 7, [314.7850036621094, 925.5916748046875, 1399.022216796875, 1140.33203125])\n",
      "('fo learn about thousands of objects from millions of images, we need a model with a large learning\\n‘apacity. However, the immense complexity of the object recognition task means that this prob-\\nem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots\\n»f prior knowledge to compensate for all the data we don’t have. Convolutional neural networks\\nCNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be con-\\nrolled by varying their depth and breadth, and they also make strong and mostly correct assumptions\\nibout the nature of images (namely, stationarity of statistics and locality of pixel dependencies).\\nThus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have\\nnuch fewer connections and parameters and so they are easier to train, while their theoretically-bes!\\nyerformance is likely to be only slightly worse.\\n', 1, [312.1918640136719, 1727.4849853515625, 1391.3443603515625, 2036.3809814453125])\n",
      "\n",
      "Question: How did the authors handle images of varying resolutions in ImageNet?\n",
      "('ImageNet consists of variable-resolution images, while our system requires a constant input dimen-\\nsionality. Therefore, we down-sampled the images to a fixed resolution of 256 x 256. Given a\\nrectangular image, we first rescaled the image such that the shorter side was of length 256, and then\\ncropped out the central 256 x 256 patch from the resulting image. We did not pre-process the images\\nin any other way, except for subtracting the mean activity over the training set from each pixel. So\\nwe trained our network on the (centered) raw RGB values of the pixels.\\n', 2, [304.58990478515625, 1459.7144775390625, 1400.64794921875, 1640.7603759765625])\n",
      "('ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000\\ncategories. The images were collected from the web and labeled by human labelers using Ama-\\nzon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object\\nChallenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge\\n(ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of\\n1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and\\n150,000 testing images.\\n', 2, [302.50250244140625, 1026.55908203125, 1395.6912841796875, 1249.4200439453125])\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "('Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture,\\nthey have still been prohibitively expensive to apply in large scale to high-resolution images. Luck-\\nily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful\\nenough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet\\ncontain enough labeled examples to train such models without severe overfitting.\\n', 2, [305.1011962890625, 230.115234375, 1399.6624755859375, 388.4068908691406])\n",
      "('fo learn about thousands of objects from millions of images, we need a model with a large learning\\n‘apacity. However, the immense complexity of the object recognition task means that this prob-\\nem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots\\n»f prior knowledge to compensate for all the data we don’t have. Convolutional neural networks\\nCNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be con-\\nrolled by varying their depth and breadth, and they also make strong and mostly correct assumptions\\nibout the nature of images (namely, stationarity of statistics and locality of pixel dependencies).\\nThus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have\\nnuch fewer connections and parameters and so they are easier to train, while their theoretically-bes!\\nyerformance is likely to be only slightly worse.\\n', 1, [312.1918640136719, 1727.4849853515625, 1391.3443603515625, 2036.3809814453125])\n",
      "iou_top1: 0.971944272518158\n",
      "2 [2, 2, 1, 2, 1]\n",
      "iou_top5: 0.971944272518158\n",
      "\n",
      "Question: What is the role of the Rectified Linear Unit (ReLU) in training deep CNNs?\n",
      "('The standard way to model a neuron’s output f as\\na function of its input x is with f(x) = tanh(zx)\\nor f(x) = (1+ e7*)~+. In terms of training time\\nwith gradient descent, these saturating nonlinearities\\nare much slower than the non-saturating nonlinearity\\nf(x) = max(0, x). Following Nair and Hinton [20],\\nwe refer to neurons with this nonlinearity as Rectified\\nLinear Units (ReLUs). Deep convolutional neural net-\\nworks with ReLUs train several times faster than their\\nequivalents with tanh units. This is demonstrated in\\nFigure 1, which shows the number of iterations re-\\nquired to reach 25% training error on the CIFAR-10\\ndataset for a particular four-layer convolutional net-\\nwork. This plot shows that we would not have been\\nable to experiment with such large neural networks for\\nthis work if we had used traditional saturating neuron\\nmodels.\\n\\n', 3, [296.0995788574219, 300.33172607421875, 919.086181640625, 831.67236328125])\n",
      "('Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture,\\nthey have still been prohibitively expensive to apply in large scale to high-resolution images. Luck-\\nily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful\\nenough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet\\ncontain enough labeled examples to train such models without severe overfitting.\\n', 2, [305.1011962890625, 230.115234375, 1399.6624755859375, 388.4068908691406])\n",
      "('ReLUs have the desirable property that they do not require input normalization to prevent them\\nfrom saturating. If at least some training examples produce a positive input to a ReLU, learning will\\nhappen in that neuron. However, we still find that the following local normalization scheme aids\\ngeneralization. Denoting by a’, y the activity of a neuron computed by applying kernel ¢ at position\\n(x,y) and then applying the ReLU nonlinearity, the response-normalized activity b’,,, is given by\\nthe expression\\n\\ny\\n\\nQ\\n', 4, [303.77734375, 284.3328552246094, 1408.745849609375, 482.5963439941406])\n",
      "('3.1 ReLU Nonlinearity\\n', 3, [295.37286376953125, 229.4542694091797, 592.419921875, 267.9515686035156])\n",
      "('We are not the first to consider alternatives to tradi-\\ntional neuron models in CNNs. For example, Jarrett\\netal. [11] claim that the nonlinearity f(x) = |tanh(z)|\\nworks particularly well with their type of contrast nor-\\nmalization followed by local average pooling on the\\nCaltech-101 dataset. However, on this dataset the pri-\\nmary concern is preventing overfitting, so the effect\\nthey are observing is different from the accelerated\\nability to fit the training set which we report when us-\\ning ReLUs. Faster learning has a great influence on the\\nperformance of large models trained on large datasets.\\n', 3, [299.12567138671875, 833.2002563476562, 902.7128295898438, 1173.1689453125])\n",
      "iou_top1: 0.0\n",
      "3 [3, 2, 4, 3, 3]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.9685190916061401\n",
      "\n",
      "Question: Why did the authors use two GPUs for training, and how was the workload distributed?\n",
      "('A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks\\nthat can be trained on it. It turns out that 1.2 million training examples are enough to train networks\\nwhich are too big to fit on one GPU. Therefore we spread the net across two GPUs. Current GPUs\\nare particularly well-suited to cross-GPU parallelization, as they are able to read from and write to\\none another’s memory directly, without going through host machine memory. The parallelization\\nscheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one\\nadditional trick: the GPUs communicate only in certain layers. This means that, for example, the\\nkernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input\\nonly from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of\\nconnectivity is a problem for cross-validation, but this allows us to precisely tune the amount of\\ncommunication until it is an acceptable fraction of the amount of computation.\\n', 3, [306.69219970703125, 1296.9888916015625, 1410.282470703125, 1635.641845703125])\n",
      "('In the end, the network’s size is limited mainly by the amount of memory available on current GPUs\\nand by the amount of training time that we are willing to tolerate. Our network takes between five\\nand six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results\\ncan be improved simply by waiting for faster GPUs and bigger datasets to become available.\\n', 2, [298.27825927734375, 783.0208740234375, 1425.000244140625, 907.9762573242188])\n",
      "('3.2. Training on Multiple GPUs\\n', 3, [307.621826171875, 1231.705322265625, 688.1534423828125, 1266.4827880859375])\n",
      "('The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Ciresan\\net al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1\\nand top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many\\nkernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time\\nto train than the one-GPU net’.\\n', 3, [300.44482421875, 1653.19921875, 1401.6768798828125, 1802.122802734375])\n",
      "('Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The\\nnetwork has learned a variety of frequency- and orientation-selective kernels, as well as various col-\\nored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connec-\\ntivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels\\non on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is\\nindependent of any particular random weight initialization (modulo a renumbering of the GPUs).\\n', 7, [303.48370361328125, 1750.81640625, 1404.93896484375, 1940.1541748046875])\n",
      "iou_top1: 0.9609779119491577\n",
      "3 [3, 2, 3, 3, 7]\n",
      "iou_top5: 0.9609779119491577\n",
      "\n",
      "Question: What property makes a Rectified Linear Unit (ReLU) desirable in learning?\n",
      "('ReLUs have the desirable property that they do not require input normalization to prevent them\\nfrom saturating. If at least some training examples produce a positive input to a ReLU, learning will\\nhappen in that neuron. However, we still find that the following local normalization scheme aids\\ngeneralization. Denoting by a’, y the activity of a neuron computed by applying kernel ¢ at position\\n(x,y) and then applying the ReLU nonlinearity, the response-normalized activity b’,,, is given by\\nthe expression\\n\\ny\\n\\nQ\\n', 4, [303.77734375, 284.3328552246094, 1408.745849609375, 482.5963439941406])\n",
      "('The standard way to model a neuron’s output f as\\na function of its input x is with f(x) = tanh(zx)\\nor f(x) = (1+ e7*)~+. In terms of training time\\nwith gradient descent, these saturating nonlinearities\\nare much slower than the non-saturating nonlinearity\\nf(x) = max(0, x). Following Nair and Hinton [20],\\nwe refer to neurons with this nonlinearity as Rectified\\nLinear Units (ReLUs). Deep convolutional neural net-\\nworks with ReLUs train several times faster than their\\nequivalents with tanh units. This is demonstrated in\\nFigure 1, which shows the number of iterations re-\\nquired to reach 25% training error on the CIFAR-10\\ndataset for a particular four-layer convolutional net-\\nwork. This plot shows that we would not have been\\nable to experiment with such large neural networks for\\nthis work if we had used traditional saturating neuron\\nmodels.\\n\\n', 3, [296.0995788574219, 300.33172607421875, 919.086181640625, 831.67236328125])\n",
      "('3.1 ReLU Nonlinearity\\n', 3, [295.37286376953125, 229.4542694091797, 592.419921875, 267.9515686035156])\n",
      "('where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total\\nnumber of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined\\nbefore training begins. This sort of response normalization implements a form of lateral inhibition\\ninspired by the type found in real neurons, creating competition for big activities amongst neuron\\noutputs computed using different kernels. The constants k,n, a, and 3 are hyper-parameters whose\\nvalues are determined using a validation set; we used k = 2,n = 5,a = 10-4, and 8 = 0.75. We\\napplied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).\\n', 4, [298.7323303222656, 586.0318603515625, 1408.369140625, 808.1409912109375])\n",
      "('The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel\\nmaps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third\\nconvolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-\\nconnected layers are connected to all neurons in the previous layer. Response-normalization layers\\nfollow the first and second convolutional layers. Max-pooling layers, of the kind described in Section\\n3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU\\nnon-linearity is applied to the output of every convolutional and fully-connected layer.\\n', 4, [291.9784851074219, 1663.8226318359375, 1407.35546875, 1880.5255126953125])\n",
      "iou_top1: 0.9206901788711548\n",
      "4 [4, 3, 3, 4, 4]\n",
      "iou_top5: 0.9206901788711548\n",
      "\n",
      "Question: How did overlapping pooling contribute to the model's performance?\n",
      "('Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel\\nmap. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g.,\\n[17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling\\nunits spaced s pixels apart, each summarizing a neighborhood of size z x z centered at the location\\nof the pooling unit. If we set s = z, we obtain traditional local pooling as commonly employed\\nin CNNs. If we set s < z, we obtain overlapping pooling. This is what we use throughout our\\nnetwork, with s = 2 and z = 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and\\n0.3%, respectively, as compared with the non-overlapping scheme s = 2,z = 2, which produces\\noutput of equivalent dimensions. We generally observe during training that models with overlapping\\npooling find it slightly more difficult to overfit.\\n', 4, [298.7404479980469, 1061.60009765625, 1414.8865966796875, 1383.6220703125])\n",
      "('3.4 Overlapping Pooling\\n', 4, [297.7735900878906, 1008.2139892578125, 611.7440185546875, 1044.8721923828125])\n",
      "('We are not the first to consider alternatives to tradi-\\ntional neuron models in CNNs. For example, Jarrett\\netal. [11] claim that the nonlinearity f(x) = |tanh(z)|\\nworks particularly well with their type of contrast nor-\\nmalization followed by local average pooling on the\\nCaltech-101 dataset. However, on this dataset the pri-\\nmary concern is preventing overfitting, so the effect\\nthey are observing is different from the accelerated\\nability to fit the training set which we report when us-\\ning ReLUs. Faster learning has a great influence on the\\nperformance of large models trained on large datasets.\\n', 3, [299.12567138671875, 833.2002563476562, 902.7128295898438, 1173.1689453125])\n",
      "('Combining the predictions of many different models is a very successful way to reduce test errors\\n[1, 3], but it appears to be too expensive for big neural networks that already take several days\\nto train. There is, however, a very efficient version of model combination that only costs about a\\nfactor of two during training. The recently-introduced technique, called “dropout” [10], consists\\nof setting to zero the output of each hidden neuron with probability 0.5. The neurons which are\\n“dropped out” in this way do not contribute to the forward pass and do not participate in back-\\npropagation. So every time an input is presented, the neural network samples a different architecture,\\nbut all these architectures share weights. This technique reduces complex co-adaptations of neurons,\\nsince a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\\nlearn more robust features that are useful in conjunction with many different random subsets of the\\nother neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a\\nreasonable approximation to taking the geometric mean of the predictive distributions produced by\\nthe exponentially-many dropout networks.\\n', 6, [301.25738525390625, 667.37939453125, 1395.9627685546875, 1076.7410888671875])\n",
      "('40.9 %, attained by the net described above but with an additional, sixth convolutional layer over th\\nlast pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].\\n', 7, [304.0283203125, 1583.1864013671875, 1385.7191162109375, 1644.9915771484375])\n",
      "iou_top1: 0.9160869717597961\n",
      "4 [4, 4, 3, 6, 7]\n",
      "iou_top5: 0.9160869717597961\n",
      "\n",
      "Question: How did the authors perform image augmentation to increase the size of the training set?\n",
      "('The first form of data augmentation consists of generating image translations and horizontal reflec-\\ntions. We do this by extracting random 224 x 224 patches (and their horizontal reflections) from the\\n256 x 256 images and training our network on these extracted patches*. This increases the size of our\\ntraining set by a factor of 2048, though the resulting training examples are, of course, highly inter-\\ndependent. Without this scheme, our network suffers from substantial overfitting, which would have\\nforced us to use much smaller networks. At test time, the network makes a prediction by extracting\\nfive 224 x 224 patches (the four corner patches and the center patch) as well as their horizontal\\nreflections (hence ten patches in all), and averaging the predictions made by the network’s softmax\\nlayer on the ten patches.\\n', 5, [303.75885009765625, 1589.916748046875, 1399.9482421875, 1861.7183837890625])\n",
      "('The second form of data augmentation consists of altering the intensities of the RGB channels in\\ntraining images. Specifically, we perform PCA on the set of RGB pixel values throughout the\\nImageNet training set. To each training image, we add multiples of the found principal components,\\n', 5, [295.6390075683594, 1883.1019287109375, 1446.39892578125, 1977.147705078125])\n",
      "('To simplify our experiments, we did not use any unsupervised pre-training even though we expect\\nthat it will help, especially if we obtain enough computational power to significantly increase the\\nsize of the network without obtaining a corresponding increase in the amount of labeled data. Thus\\nfar, our results have improved as we have made our network larger and trained it longer but we still\\nhave many orders of magnitude to go in order to match the infero-temporal pathway of the human\\nvisual system. Ultimately we would like to use very large and deep convolutional nets on video\\nsequences where the temporal structure provides very helpful information that is missing or far less\\nobvious in static images.\\n', 8, [299.36651611328125, 1785.95458984375, 1400.3897705078125, 2035.8184814453125])\n",
      "('The easiest and most common method to reduce overfitting on image data is to artificially enlarge\\nthe dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms\\nof data augmentation, both of which allow transformed images to be produced from the original\\nimages with very little computation, so the transformed images do not need to be stored on disk.\\nIn our implementation, the transformed images are generated in Python code on the CPU while the\\nGPU is training on the previous batch of images. So these data augmentation schemes are, in effect,\\ncomputationally free.\\n', 5, [301.68780517578125, 1363.1085205078125, 1403.873291015625, 1582.4873046875])\n",
      "('ImageNet consists of variable-resolution images, while our system requires a constant input dimen-\\nsionality. Therefore, we down-sampled the images to a fixed resolution of 256 x 256. Given a\\nrectangular image, we first rescaled the image such that the shorter side was of length 256, and then\\ncropped out the central 256 x 256 patch from the resulting image. We did not pre-process the images\\nin any other way, except for subtracting the mean activity over the training set from each pixel. So\\nwe trained our network on the (centered) raw RGB values of the pixels.\\n', 2, [304.58990478515625, 1459.7144775390625, 1400.64794921875, 1640.7603759765625])\n",
      "iou_top1: 0.9554189443588257\n",
      "5 [5, 5, 8, 5, 2]\n",
      "iou_top5: 0.9554189443588257\n",
      "\n",
      "Question: How does dropout help a neural network to learn better?\n",
      "('Combining the predictions of many different models is a very successful way to reduce test errors\\n[1, 3], but it appears to be too expensive for big neural networks that already take several days\\nto train. There is, however, a very efficient version of model combination that only costs about a\\nfactor of two during training. The recently-introduced technique, called “dropout” [10], consists\\nof setting to zero the output of each hidden neuron with probability 0.5. The neurons which are\\n“dropped out” in this way do not contribute to the forward pass and do not participate in back-\\npropagation. So every time an input is presented, the neural network samples a different architecture,\\nbut all these architectures share weights. This technique reduces complex co-adaptations of neurons,\\nsince a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\\nlearn more robust features that are useful in conjunction with many different random subsets of the\\nother neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a\\nreasonable approximation to taking the geometric mean of the predictive distributions produced by\\nthe exponentially-many dropout networks.\\n', 6, [301.25738525390625, 667.37939453125, 1395.9627685546875, 1076.7410888671875])\n",
      "('We use dropout in the first two fully-connected layers of Figure 2. Without dropout, our network ex-\\nhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge.\\n', 6, [285.4482421875, 1086.260009765625, 1429.4007568359375, 1148.740234375])\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "('Our results show that a large, deep convolutional neural network is capable of achieving record-\\nbreaking results on a highly challenging dataset using purely supervised learning. It is notable\\nthat our network’s performance degrades if a single convolutional layer is removed. For example,\\nremoving any of the middle layers results in a loss of about 2% for the top-1 performance of the\\nnetwork. So the depth really is important for achieving our results.\\n', 8, [305.5085754394531, 1620.9405517578125, 1402.94189453125, 1778.2506103515625])\n",
      "('Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC\\nmake each training example impose 10 bits of constraint on the mapping from image to label, this\\nturns out to be insufficient to learn so many parameters without considerable overfitting. Below, we\\ndescribe the two primary ways in which we combat overfitting.\\n', 5, [307.04473876953125, 1137.6920166015625, 1399.484375, 1271.07958984375])\n",
      "iou_top1: 0.9521914720535278\n",
      "6 [6, 6, 1, 8, 5]\n",
      "iou_top5: 0.9521914720535278\n",
      "\n",
      "Question: What hardware was used for training, and for how long?\n",
      "('reduced three times prior to termination. We trained the network for roughly 90 cycles through the\\ntraining set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.\\n', 7, [297.3795166015625, 231.96038818359375, 1415.432373046875, 294.838623046875])\n",
      "('In the end, the network’s size is limited mainly by the amount of memory available on current GPUs\\nand by the amount of training time that we are willing to tolerate. Our network takes between five\\nand six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results\\ncan be improved simply by waiting for faster GPUs and bigger datasets to become available.\\n', 2, [298.27825927734375, 783.0208740234375, 1425.000244140625, 907.9762573242188])\n",
      "('3.2. Training on Multiple GPUs\\n', 3, [307.621826171875, 1231.705322265625, 688.1534423828125, 1266.4827880859375])\n",
      "('To simplify our experiments, we did not use any unsupervised pre-training even though we expect\\nthat it will help, especially if we obtain enough computational power to significantly increase the\\nsize of the network without obtaining a corresponding increase in the amount of labeled data. Thus\\nfar, our results have improved as we have made our network larger and trained it longer but we still\\nhave many orders of magnitude to go in order to match the infero-temporal pathway of the human\\nvisual system. Ultimately we would like to use very large and deep convolutional nets on video\\nsequences where the temporal structure provides very helpful information that is missing or far less\\nobvious in static images.\\n', 8, [299.36651611328125, 1785.95458984375, 1400.3897705078125, 2035.8184814453125])\n",
      "('The specific contributions of this paper are as follows: we trained one of the largest convolutional\\nneural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012\\ncompetitions [2] and achieved by far the best results ever reported on these datasets. We wrote a\\nhighly-optimized GPU implementation of 2D convolution and all the other operations inherent in\\ntraining convolutional neural networks, which we make available publicly!. Our network contains\\na number of new and unusual features which improve its performance and reduce its training time,\\nwhich are detailed in Section 3. The size of our network made overfitting a significant problem, even\\nwith 1.2 million labeled training examples, so we used several effective techniques for preventing\\noverfitting, which are described in Section 4. Our final network contains five convolutional and\\nthree fully-connected layers, and this depth seems to be important: we found that removing any\\nconvolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in\\ninferior performance.\\n', 2, [306.3596496582031, 395.00555419921875, 1406.600830078125, 774.2710571289062])\n",
      "2 [7, 2, 3, 8, 2]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.0\n",
      "\n",
      "Question: What performance did the CNN achieve on the ILSVRC-2010 test set?\n",
      "('Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5\\ntest set error rates of 37.5% and 17.0%>. The best performance achieved during the ILSVRC-\\n2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced\\nfrom six sparse-coding models trained on different features [2], and since then the best pub-\\nlished results are 45.7% and 25.7% with an approach that averages the predictions of two classi-\\nfiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\\n', 7, [298.6658020019531, 422.068603515625, 1409.2261962890625, 612.32958984375])\n",
      "('We also entered our model in the ILSVRC-2012 com-\\npetition and report our results in Table 2. Since the\\nILSVRC-2012 test set labels are not publicly available,\\nwe cannot report test error rates for all the models that\\nwe tried. In the remainder of this paragraph, we use\\nvalidation and test error rates interchangeably because\\nin our experience they do not differ by more than 0.1%\\n(see Table 2). The CNN described in this paper achieves\\na top-5 error rate of 18.2%. Averaging the predictions\\n', 7, [296.1936340332031, 649.3339233398438, 932.305908203125, 919.6984252929688])\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "('yf five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth con-\\nolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release\\n15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of\\n6.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 re-\\nease with the aforementioned five CNNs gives an error rate of 15.3%. The second-best con-\\nest entry achieved an error rate of 26.2% with an approach that averages the predictions of sev-\\nral classifiers trained on FVs computed from different types of densely-sampled features [7].\\n', 7, [314.7850036621094, 925.5916748046875, 1399.022216796875, 1140.33203125])\n",
      "iou_top1: 0.9247679710388184\n",
      "7 [7, 7, 1, 1, 7]\n",
      "iou_top5: 0.9247679710388184\n",
      "\n",
      "Question: How can similarity computation between high-dimensional vectors made more efficient?\n",
      "('Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vec-\\ntors is inefficient, but it could be made efficient by training an auto-encoder to compress these vectors\\nto short binary codes. This should produce a much better image retrieval method than applying auto-\\nencoders to the raw pixels [14], which does not make use of image labels and hence has a tendency\\nto retrieve images with similar patterns of edges, whether or not they are semantically similar.\\n', 8, [295.8056640625, 1329.287841796875, 1408.198486328125, 1485.2254638671875])\n",
      "('Another way to probe the network’s visual knowledge is to consider the feature activations induced\\nby an image at the last, 4096-dimensional hidden layer. If two images produce feature activatior\\nvectors with a small Euclidean separation, we can say that the higher levels of the neural network\\nconsider them to be similar. Figure 4 shows five images from the test set and the six images from\\nthe training set that are most similar to each of them according to this measure. Notice that at the\\npixel level, the retrieved training images are generally not close in L2 to the query images in the firs!\\ncolumn. For example, the retrieved dogs and elephants appear in a variety of poses. We present the\\nresults for many more test images in the supplementary material.\\n', 8, [306.0493469238281, 1061.9461669921875, 1392.551025390625, 1316.3692626953125])\n",
      "('Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture,\\nthey have still been prohibitively expensive to apply in large scale to high-resolution images. Luck-\\nily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful\\nenough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet\\ncontain enough labeled examples to train such models without severe overfitting.\\n', 2, [305.1011962890625, 230.115234375, 1399.6624755859375, 388.4068908691406])\n",
      "('Combining the predictions of many different models is a very successful way to reduce test errors\\n[1, 3], but it appears to be too expensive for big neural networks that already take several days\\nto train. There is, however, a very efficient version of model combination that only costs about a\\nfactor of two during training. The recently-introduced technique, called “dropout” [10], consists\\nof setting to zero the output of each hidden neuron with probability 0.5. The neurons which are\\n“dropped out” in this way do not contribute to the forward pass and do not participate in back-\\npropagation. So every time an input is presented, the neural network samples a different architecture,\\nbut all these architectures share weights. This technique reduces complex co-adaptations of neurons,\\nsince a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\\nlearn more robust features that are useful in conjunction with many different random subsets of the\\nother neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a\\nreasonable approximation to taking the geometric mean of the predictive distributions produced by\\nthe exponentially-many dropout networks.\\n', 6, [301.25738525390625, 667.37939453125, 1395.9627685546875, 1076.7410888671875])\n",
      "('S.C. Turaga, J.F. Murray, V. Jain, F Roth, M. Helmstaedter, K. Briggman, W. Denk, and H.S. Seung. Con-\\nvolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation,\\n\\n', 9, [348.9269714355469, 1884.0474853515625, 1395.2685546875, 1942.1944580078125])\n",
      "iou_top1: 0.9325111508369446\n",
      "8 [8, 8, 2, 6, 9]\n",
      "iou_top5: 0.9325111508369446\n",
      "\n",
      "Question: How is the network's learned knowledge examined?\n",
      "('In the left panel of Figure 4 we qualitatively assess what the network has learned by computing its\\ntop-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the\\ntop-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example,\\nonly other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry)\\nthere is genuine ambiguity about the intended focus of the photograph.\\n', 8, [304.6097412109375, 898.4732055664062, 1400.3326416015625, 1049.205322265625])\n",
      "('The architecture of our network is summarized in Figure 2. It contains eight learned layers —\\nfive convolutional and three fully-connected. Below, we describe some of the novel or unusual\\nfeatures of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of\\ntheir importance, with the most important first.\\n', 2, [302.83001708984375, 1769.8778076171875, 1399.395751953125, 1890.74658203125])\n",
      "('Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The\\nnetwork has learned a variety of frequency- and orientation-selective kernels, as well as various col-\\nored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connec-\\ntivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels\\non on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is\\nindependent of any particular random weight initialization (modulo a renumbering of the GPUs).\\n', 7, [303.48370361328125, 1750.81640625, 1404.93896484375, 1940.1541748046875])\n",
      "('Another way to probe the network’s visual knowledge is to consider the feature activations induced\\nby an image at the last, 4096-dimensional hidden layer. If two images produce feature activatior\\nvectors with a small Euclidean separation, we can say that the higher levels of the neural network\\nconsider them to be similar. Figure 4 shows five images from the test set and the six images from\\nthe training set that are most similar to each of them according to this measure. Notice that at the\\npixel level, the retrieved training images are generally not close in L2 to the query images in the firs!\\ncolumn. For example, the retrieved dogs and elephants appear in a variety of poses. We present the\\nresults for many more test images in the supplementary material.\\n', 8, [306.0493469238281, 1061.9461669921875, 1392.551025390625, 1316.3692626953125])\n",
      "('Our results show that a large, deep convolutional neural network is capable of achieving record-\\nbreaking results on a highly challenging dataset using purely supervised learning. It is notable\\nthat our network’s performance degrades if a single convolutional layer is removed. For example,\\nremoving any of the middle layers results in a loss of about 2% for the top-1 performance of the\\nnetwork. So the depth really is important for achieving our results.\\n', 8, [305.5085754394531, 1620.9405517578125, 1402.94189453125, 1778.2506103515625])\n",
      "iou_top1: 0.0\n",
      "8 [8, 2, 7, 8, 8]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.9312132596969604\n",
      "\n",
      "Question: What effect did depth have on network performance?\n",
      "('Our results show that a large, deep convolutional neural network is capable of achieving record-\\nbreaking results on a highly challenging dataset using purely supervised learning. It is notable\\nthat our network’s performance degrades if a single convolutional layer is removed. For example,\\nremoving any of the middle layers results in a loss of about 2% for the top-1 performance of the\\nnetwork. So the depth really is important for achieving our results.\\n', 8, [305.5085754394531, 1620.9405517578125, 1402.94189453125, 1778.2506103515625])\n",
      "('The specific contributions of this paper are as follows: we trained one of the largest convolutional\\nneural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012\\ncompetitions [2] and achieved by far the best results ever reported on these datasets. We wrote a\\nhighly-optimized GPU implementation of 2D convolution and all the other operations inherent in\\ntraining convolutional neural networks, which we make available publicly!. Our network contains\\na number of new and unusual features which improve its performance and reduce its training time,\\nwhich are detailed in Section 3. The size of our network made overfitting a significant problem, even\\nwith 1.2 million labeled training examples, so we used several effective techniques for preventing\\noverfitting, which are described in Section 4. Our final network contains five convolutional and\\nthree fully-connected layers, and this depth seems to be important: we found that removing any\\nconvolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in\\ninferior performance.\\n', 2, [306.3596496582031, 395.00555419921875, 1406.600830078125, 774.2710571289062])\n",
      "('Figure 1: A four-layer convolutional neural\\nnetwork with ReLUs (solid line) reaches a 25%\\ntraining error rate on CIFAR-10 six times faster\\nthan an equivalent network with tanh neurons\\n(dashed line). The learning rates for each net-\\nwork were chosen independently to make train-\\ning as fast as possible. No regularization of\\nany kind was employed. The magnitude of the\\neffect demonstrated here varies with network\\narchitecture, but networks with ReLUs consis-\\ntently learn several times faster than equivalents\\nwith saturating neurons.\\n', 3, [923.4597778320312, 761.7416381835938, 1405.3201904296875, 1120.8250732421875])\n",
      "('To simplify our experiments, we did not use any unsupervised pre-training even though we expect\\nthat it will help, especially if we obtain enough computational power to significantly increase the\\nsize of the network without obtaining a corresponding increase in the amount of labeled data. Thus\\nfar, our results have improved as we have made our network larger and trained it longer but we still\\nhave many orders of magnitude to go in order to match the infero-temporal pathway of the human\\nvisual system. Ultimately we would like to use very large and deep convolutional nets on video\\nsequences where the temporal structure provides very helpful information that is missing or far less\\nobvious in static images.\\n', 8, [299.36651611328125, 1785.95458984375, 1400.3897705078125, 2035.8184814453125])\n",
      "('In the end, the network’s size is limited mainly by the amount of memory available on current GPUs\\nand by the amount of training time that we are willing to tolerate. Our network takes between five\\nand six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results\\ncan be improved simply by waiting for faster GPUs and bigger datasets to become available.\\n', 2, [298.27825927734375, 783.0208740234375, 1425.000244140625, 907.9762573242188])\n",
      "iou_top1: 0.9318888187408447\n",
      "8 [8, 2, 3, 8, 2]\n",
      "iou_top5: 0.9318888187408447\n",
      "\n",
      "Question: How many neurons does each fully connected layer in the model have?\n",
      "('neurons in a kernel map). The second convolutional layer takes as input the (response-normalized\\nand pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 x 5 x 48.\\nThe third, fourth, and fifth convolutional layers are connected to one another without any intervening\\npooling or normalization layers. The third convolutional layer has 384 kernels of size 3 x 3 x\\n256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth\\nconvolutional layer has 384 kernels of size 3 x 3 x 192, and the fifth convolutional layer has 256\\nkernels of size 3 x 3 x 192. The fully-connected layers have 4096 neurons each.\\n', 5, [299.0901794433594, 803.8162841796875, 1401.75732421875, 1027.02587890625])\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "('The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel\\nmaps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third\\nconvolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-\\nconnected layers are connected to all neurons in the previous layer. Response-normalization layers\\nfollow the first and second convolutional layers. Max-pooling layers, of the kind described in Section\\n3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU\\nnon-linearity is applied to the output of every convolutional and fully-connected layer.\\n', 4, [291.9784851074219, 1663.8226318359375, 1407.35546875, 1880.5255126953125])\n",
      "('We initialized the weights in each layer from a zero-mean Gaussian distribution with standard de-\\nviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers,\\nas well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates\\nthe early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron\\nbiases in the remaining layers with the constant 0.\\n', 6, [297.77728271484375, 1773.7994384765625, 1404.46142578125, 1924.8841552734375])\n",
      "('Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net\\ncontains eight layers with weights; the first five are convolutional and the remaining three are fully-\\nconnected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces\\na distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression\\nobjective, which is equivalent to maximizing the average across training cases of the log-probability\\nof the correct label under the prediction distribution.\\n', 4, [303.6824035644531, 1467.699462890625, 1400.0052490234375, 1653.4893798828125])\n",
      "iou_top1: 0.9298659563064575\n",
      "5 [5, 1, 4, 6, 4]\n",
      "iou_top5: 0.9298659563064575\n",
      "\n",
      "Question: What future improvements did the authors suggest?\n",
      "('To simplify our experiments, we did not use any unsupervised pre-training even though we expect\\nthat it will help, especially if we obtain enough computational power to significantly increase the\\nsize of the network without obtaining a corresponding increase in the amount of labeled data. Thus\\nfar, our results have improved as we have made our network larger and trained it longer but we still\\nhave many orders of magnitude to go in order to match the infero-temporal pathway of the human\\nvisual system. Ultimately we would like to use very large and deep convolutional nets on video\\nsequences where the temporal structure provides very helpful information that is missing or far less\\nobvious in static images.\\n', 8, [299.36651611328125, 1785.95458984375, 1400.3897705078125, 2035.8184814453125])\n",
      "('References\\n\\n1\\n\\noO\\n\\nR.M. Bell and Y. Koren. Lessons from the netflix prize challenge. ACM SIGKDD Explorations Newsletter,\\n9(2):75-79, 2007.\\n\\nA. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.image-\\nnet.org/challenges. 2010.\\n\\nL. Breiman. Random forests. Machine learning, 45(1):5-32, 2001.\\n\\nD. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification.\\nArxiv preprint arXiv:1202.2745, 2012.\\n\\nD.C. Ciresan, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural\\nnetworks for visual object classification. Arxiv preprint arXiv: 1102.0183, 2011.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\\nImage Database. In CVPRO9, 2009.\\n\\nJ. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. JLSVRC-2012, 2012. URL\\nhttp://www. image-net.org/challenges/LSVRC/2012/.\\n\\nL. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An\\nincremental bayesian approach tested on 101 object categories. Computer Vision and Image Understand-\\ning, 106(1):59-70, 2007.\\n\\nG. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, Cali-\\nfornia Institute of Technology, 2007. URL http: //authors.library.caltech.edu/7694.\\nGE. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural net-\\nworks by preventing co-adaptation of feature detectors. arXiv preprint arXiv: 1207.0580, 2012.\\n\\nK. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y. LeCun. What is the best multi-stage architecture for\\nobject recognition? In Jnternational Conference on Computer Vision, pages 2146-2153. IEEE, 2009.\\n\\nA. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of\\nComputer Science, University of Toronto, 2009.\\n\\nA. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.\\n\\nA. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In\\nESANN, 2011.\\n\\nYY. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Hand-\\nwritten digit recognition with a back-propagation network. In Advances in neural information processing\\nsystems, 1990.\\n\\nY. LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to\\npose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the\\n2004 IEEE Computer Society Conference on, volume 2, pages I-97. IEEE, 2004.\\n\\nY. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In\\nCircuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253-256.\\nIEEE, 2010.\\n\\nH. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable unsuper-\\nvised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference\\non Machine Learning, pages 609-616. ACM, 2009.\\n\\nT. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classifi-\\ncation: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer\\nVision, Florence, Italy, October 2012.\\n\\nV. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th\\nInternational Conference on Machine Learning, 2010.\\n\\nN. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computa-\\ntional biology, 4(1):e27, 2008.\\n\\nN. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering\\ngood forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579,\\n2009.\\n\\nB.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for\\nimage annotation. International journal of computer vision, 77(1):157—173, 2008.\\n\\nJ. Sanchez and F. Perronnin. High-dimensional signature compression for large-scale image classification.\\nIn Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665-1672. IEEE,\\n2011.\\n\\nP.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to\\nvisual document analysis. In Proceedings of the Seventh International Conference on Document Analysis\\nand Recognition, volume 2, pages 958-962, 2003.\\n\\nS.C. Turaga, J.F. Murray, V. Jain, F Roth, M. Helmstaedter, K. Briggman, W. Denk, and H.S. Seung. Con-\\nvolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation,\\n22(2):511-538, 2010.\\n', 9, [307.64892578125, 235.37535095214844, 1423.6895751953125, 1986.3394775390625])\n",
      "('The specific contributions of this paper are as follows: we trained one of the largest convolutional\\nneural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012\\ncompetitions [2] and achieved by far the best results ever reported on these datasets. We wrote a\\nhighly-optimized GPU implementation of 2D convolution and all the other operations inherent in\\ntraining convolutional neural networks, which we make available publicly!. Our network contains\\na number of new and unusual features which improve its performance and reduce its training time,\\nwhich are detailed in Section 3. The size of our network made overfitting a significant problem, even\\nwith 1.2 million labeled training examples, so we used several effective techniques for preventing\\noverfitting, which are described in Section 4. Our final network contains five convolutional and\\nthree fully-connected layers, and this depth seems to be important: we found that removing any\\nconvolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in\\ninferior performance.\\n', 2, [306.3596496582031, 395.00555419921875, 1406.600830078125, 774.2710571289062])\n",
      "('In the end, the network’s size is limited mainly by the amount of memory available on current GPUs\\nand by the amount of training time that we are willing to tolerate. Our network takes between five\\nand six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results\\ncan be improved simply by waiting for faster GPUs and bigger datasets to become available.\\n', 2, [298.27825927734375, 783.0208740234375, 1425.000244140625, 907.9762573242188])\n",
      "('Combining the predictions of many different models is a very successful way to reduce test errors\\n[1, 3], but it appears to be too expensive for big neural networks that already take several days\\nto train. There is, however, a very efficient version of model combination that only costs about a\\nfactor of two during training. The recently-introduced technique, called “dropout” [10], consists\\nof setting to zero the output of each hidden neuron with probability 0.5. The neurons which are\\n“dropped out” in this way do not contribute to the forward pass and do not participate in back-\\npropagation. So every time an input is presented, the neural network samples a different architecture,\\nbut all these architectures share weights. This technique reduces complex co-adaptations of neurons,\\nsince a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\\nlearn more robust features that are useful in conjunction with many different random subsets of the\\nother neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a\\nreasonable approximation to taking the geometric mean of the predictive distributions produced by\\nthe exponentially-many dropout networks.\\n', 6, [301.25738525390625, 667.37939453125, 1395.9627685546875, 1076.7410888671875])\n",
      "iou_top1: 0.9562134742736816\n",
      "8 [8, 9, 2, 2, 6]\n",
      "iou_top5: 0.9562134742736816\n",
      "\n",
      "Question: What are the main advantages of convolutional networks over traditional feedforward networks?\n",
      "('fo learn about thousands of objects from millions of images, we need a model with a large learning\\n‘apacity. However, the immense complexity of the object recognition task means that this prob-\\nem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots\\n»f prior knowledge to compensate for all the data we don’t have. Convolutional neural networks\\nCNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be con-\\nrolled by varying their depth and breadth, and they also make strong and mostly correct assumptions\\nibout the nature of images (namely, stationarity of statistics and locality of pixel dependencies).\\nThus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have\\nnuch fewer connections and parameters and so they are easier to train, while their theoretically-bes!\\nyerformance is likely to be only slightly worse.\\n', 1, [312.1918640136719, 1727.4849853515625, 1391.3443603515625, 2036.3809814453125])\n",
      "('The specific contributions of this paper are as follows: we trained one of the largest convolutional\\nneural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012\\ncompetitions [2] and achieved by far the best results ever reported on these datasets. We wrote a\\nhighly-optimized GPU implementation of 2D convolution and all the other operations inherent in\\ntraining convolutional neural networks, which we make available publicly!. Our network contains\\na number of new and unusual features which improve its performance and reduce its training time,\\nwhich are detailed in Section 3. The size of our network made overfitting a significant problem, even\\nwith 1.2 million labeled training examples, so we used several effective techniques for preventing\\noverfitting, which are described in Section 4. Our final network contains five convolutional and\\nthree fully-connected layers, and this depth seems to be important: we found that removing any\\nconvolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in\\ninferior performance.\\n', 2, [306.3596496582031, 395.00555419921875, 1406.600830078125, 774.2710571289062])\n",
      "('Our results show that a large, deep convolutional neural network is capable of achieving record-\\nbreaking results on a highly challenging dataset using purely supervised learning. It is notable\\nthat our network’s performance degrades if a single convolutional layer is removed. For example,\\nremoving any of the middle layers results in a loss of about 2% for the top-1 performance of the\\nnetwork. So the depth really is important for achieving our results.\\n', 8, [305.5085754394531, 1620.9405517578125, 1402.94189453125, 1778.2506103515625])\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "('Figure 1: A four-layer convolutional neural\\nnetwork with ReLUs (solid line) reaches a 25%\\ntraining error rate on CIFAR-10 six times faster\\nthan an equivalent network with tanh neurons\\n(dashed line). The learning rates for each net-\\nwork were chosen independently to make train-\\ning as fast as possible. No regularization of\\nany kind was employed. The magnitude of the\\neffect demonstrated here varies with network\\narchitecture, but networks with ReLUs consis-\\ntently learn several times faster than equivalents\\nwith saturating neurons.\\n', 3, [923.4597778320312, 761.7416381835938, 1405.3201904296875, 1120.8250732421875])\n",
      "iou_top1: 0.9552580714225769\n",
      "1 [1, 2, 8, 1, 3]\n",
      "iou_top5: 0.9552580714225769\n",
      "\n",
      "Question: How were the model weights initialized before training?\n",
      "('We initialized the weights in each layer from a zero-mean Gaussian distribution with standard de-\\nviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers,\\nas well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates\\nthe early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron\\nbiases in the remaining layers with the constant 0.\\n', 6, [297.77728271484375, 1773.7994384765625, 1404.46142578125, 1924.8841552734375])\n",
      "('We trained our models using stochastic gradient descent\\nwith a batch size of 128 examples, momentum of 0.9, and\\nweight decay of 0.0005. We found that this small amount\\nof weight decay was important for the model to learn. In\\nother words, weight decay here is not merely a regularizer:\\nit reduces the model’s training error. The update rule for\\nweight w was\\n', 6, [293.39300537109375, 1289.83203125, 944.4925537109375, 1504.9766845703125])\n",
      "('Combining the predictions of many different models is a very successful way to reduce test errors\\n[1, 3], but it appears to be too expensive for big neural networks that already take several days\\nto train. There is, however, a very efficient version of model combination that only costs about a\\nfactor of two during training. The recently-introduced technique, called “dropout” [10], consists\\nof setting to zero the output of each hidden neuron with probability 0.5. The neurons which are\\n“dropped out” in this way do not contribute to the forward pass and do not participate in back-\\npropagation. So every time an input is presented, the neural network samples a different architecture,\\nbut all these architectures share weights. This technique reduces complex co-adaptations of neurons,\\nsince a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\\nlearn more robust features that are useful in conjunction with many different random subsets of the\\nother neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a\\nreasonable approximation to taking the geometric mean of the predictive distributions produced by\\nthe exponentially-many dropout networks.\\n', 6, [301.25738525390625, 667.37939453125, 1395.9627685546875, 1076.7410888671875])\n",
      "('We used an equal learning rate for all layers, which we adjusted manually throughout training.\\nThe heuristic which we followed was to divide the learning rate by 10 when the validation error\\nrate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and\\n', 6, [297.59112548828125, 1943.126953125, 1405.8065185546875, 2035.3673095703125])\n",
      "('ImageNet consists of variable-resolution images, while our system requires a constant input dimen-\\nsionality. Therefore, we down-sampled the images to a fixed resolution of 256 x 256. Given a\\nrectangular image, we first rescaled the image such that the shorter side was of length 256, and then\\ncropped out the central 256 x 256 patch from the resulting image. We did not pre-process the images\\nin any other way, except for subtracting the mean activity over the training set from each pixel. So\\nwe trained our network on the (centered) raw RGB values of the pixels.\\n', 2, [304.58990478515625, 1459.7144775390625, 1400.64794921875, 1640.7603759765625])\n",
      "iou_top1: 0.964031994342804\n",
      "6 [6, 6, 6, 6, 2]\n",
      "iou_top5: 0.964031994342804\n",
      "\n",
      "Question: How many images does the ImageNet dataset have??\n",
      "('ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000\\ncategories. The images were collected from the web and labeled by human labelers using Ama-\\nzon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object\\nChallenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge\\n(ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of\\n1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and\\n150,000 testing images.\\n', 2, [302.50250244140625, 1026.55908203125, 1395.6912841796875, 1249.4200439453125])\n",
      "('ImageNet consists of variable-resolution images, while our system requires a constant input dimen-\\nsionality. Therefore, we down-sampled the images to a fixed resolution of 256 x 256. Given a\\nrectangular image, we first rescaled the image such that the shorter side was of length 256, and then\\ncropped out the central 256 x 256 patch from the resulting image. We did not pre-process the images\\nin any other way, except for subtracting the mean activity over the training set from each pixel. So\\nwe trained our network on the (centered) raw RGB values of the pixels.\\n', 2, [304.58990478515625, 1459.7144775390625, 1400.64794921875, 1640.7603759765625])\n",
      "('We trained a large, deep convolutional neural network to classify the 1.2 million\\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\\nand 17.0% which is considerably better than the previous state-of-the-art. The\\nneural network, which has 60 million parameters and 650,000 neurons, consists\\nof five convolutional layers, some of which are followed by max-pooling layers,\\nand three fully-connected layers with a final 1000-way softmax. To make train-\\ning faster, we used non-saturating neurons and a very efficient GPU implemen-\\ntation of the convolution operation. To reduce overfitting in the fully-connected\\nlayers we employed a recently-developed regularization method called “dropout”\\nthat proved to be very effective. We also entered a variant of this model in the\\nILSVRC-2012 competition and achieved a winning top-S test error rate of 15.3%,\\ncompared to 26.2% achieved by the second-best entry.\\n', 1, [401.020751953125, 762.2279663085938, 1299.40087890625, 1166.69140625])\n",
      "('Current approaches to object recognition make essential use of machine learning methods. To im-\\nprove their performance, we can collect larger datasets, learn more powerful models, and use bet-\\nter techniques for preventing overfitting. Until recently, datasets of labeled images were relatively\\nsmall — on the order of tens of thousands of images (e.g., NORB [16], Caltech- 101/256 [8, 9], and\\nCIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size,\\nespecially if they are augmented with label-preserving transformations. For example, the current-\\nbest error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].\\nBut objects in realistic settings exhibit considerable variability, so to learn to recognize them it is\\nnecessary to use much larger training sets. And indeed, the shortcomings of small image datasets\\nhave been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to col-\\nlect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which\\nconsists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of\\nover 15 million labeled high-resolution images in over 22,000 categories.\\n', 1, [304.52996826171875, 1312.4246826171875, 1403.270751953125, 1715.24169921875])\n",
      "('Current approaches to object recognition make essential use of machine learning methods. To im-\\nprove their performance, we can collect larger datasets, learn more powerful models, and use bet-\\nter techniques for preventing overfitting. Until recently, datasets of labeled images were relatively\\nsmall — on the order of tens of thousands of images (e.g., NORB [16], Caltech- 101/256 [8, 9], and\\nCIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size,\\nespecially if they are augmented with label-preserving transformations. For example, the current-\\nbest error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].\\nBut objects in realistic settings exhibit considerable variability, so to learn to recognize them it is\\nnecessary to use much larger training sets. And indeed, the shortcomings of small image datasets\\nhave been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to col-\\nlect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which\\nconsists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of\\nover 15 million labeled high-resolution images in over 22,000 categories.\\n', 1, [304.52996826171875, 1312.4246826171875, 1403.270751953125, 1715.24169921875])\n",
      "iou_top1: 0.9354180097579956\n",
      "2 [2, 2, 1, 1, 1]\n",
      "iou_top5: 0.9354180097579956\n",
      "Top-1\n",
      "Correct pages: 0.9\n",
      "Correct regions: 0.8, IoU: 0.9396701455116272\n",
      "Top-5\n",
      "Correct pages: 0.95\n",
      "Correct regions: 0.9, IoU: 0.9408029913902283\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current doc: 2010.11929v2.pdf\n",
      "\n",
      "Question: How do the authors apply the Transformer architecture to image recognition?\n",
      "('We have explored the direct application of Transformers to image recognition. Unlike prior works\\nusing self-attention in computer vision, we do not introduce image-specific inductive biases into\\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classification\\ndatasets, whilst being relatively cheap to pre-train.\\n', 9, [300.0313415527344, 1146.4471435546875, 1412.639404296875, 1365.279052734375])\n",
      "('While the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classification tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.!\\n', 1, [394.4130859375, 746.5789184570312, 1302.3428955078125, 1086.92041015625])\n",
      "('Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modifications. To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\\nthe model on image classification in supervised fashion.\\n', 1, [299.5771484375, 1684.71337890625, 1397.6695556640625, 1837.021728515625])\n",
      "('In order to stay as close as possible to the original Transformer model, we made use of an additional\\n\\n[class] token, which is taken as image representation. The output of this token is then trans-\\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\\nin the single hidden layer.\\n', 16, [303.8602294921875, 1772.33203125, 1400.0115966796875, 1899.2979736328125])\n",
      "('Figure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classification, we use the standard approach of adding an extra learnable\\n“classification token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n', 3, [298.2052307128906, 732.6156616210938, 1400.2398681640625, 894.8108520507812])\n",
      "1 [9, 1, 1, 16, 3]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.9647682905197144\n",
      "\n",
      "Question: Compared to NLP, how was attention applied for vision tasks?\n",
      "('While the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classification tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.!\\n', 1, [394.4130859375, 746.5789184570312, 1302.3428955078125, 1086.92041015625])\n",
      "('We have explored the direct application of Transformers to image recognition. Unlike prior works\\nusing self-attention in computer vision, we do not introduce image-specific inductive biases into\\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classification\\ndatasets, whilst being relatively cheap to pre-train.\\n', 9, [300.0313415527344, 1146.4471435546875, 1412.639404296875, 1365.279052734375])\n",
      "('In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\\ntheoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to\\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\\n2020).\\n', 1, [303.58251953125, 1422.8336181640625, 1394.7333984375, 1671.193603515625])\n",
      "('Naive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefficiently on hardware accelerators.\\n', 2, [303.70391845703125, 890.9014892578125, 1410.692138671875, 1262.37744140625])\n",
      "('Naive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefficiently on hardware accelerators.\\n', 2, [303.70391845703125, 890.9014892578125, 1410.692138671875, 1262.37744140625])\n",
      "iou_top1: 0.0\n",
      "1 [1, 9, 1, 2, 2]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.0\n",
      "\n",
      "Question: What datasets were used to train and benchmark the model?\n",
      "('Datasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\\nfollows Kolesnikov et al. (2020).\\n', 4, [305.16485595703125, 1756.924560546875, 1400.248046875, 2031.591064453125])\n",
      "('Datasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\\nfollows Kolesnikov et al. (2020).\\n', 4, [305.16485595703125, 1756.924560546875, 1400.248046875, 2031.591064453125])\n",
      "('We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n', 4, [299.4459533691406, 1479.2294921875, 1404.611572265625, 1669.601806640625])\n",
      "('However, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nfind that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n', 2, [306.51556396484375, 308.9344787597656, 1405.1368408203125, 526.0833129882812])\n",
      "('Our work adds to the increasing collection of papers that explore image recognition at larger scales\\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\\nwell, but train Transformers instead of ResNet-based models used in prior works.\\n', 2, [300.60992431640625, 1818.3189697265625, 1399.284423828125, 2042.183837890625])\n",
      "iou_top1: 0.9709559679031372\n",
      "4 [4, 4, 4, 2, 2]\n",
      "iou_top5: 0.9709559679031372\n",
      "\n",
      "Question: Why do the authors intentionally keep the setup to be simple?\n",
      "('n model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures — and\\nheir efficient implementations — can be used almost out of the box.\\n', 3, [311.0866394042969, 1097.1317138671875, 1401.644287109375, 1193.96240234375])\n",
      "('for ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\\nthis run and our sweep. Finally, if not mentioned otherwise, all fine-tuning experiments run at 384\\nresolution (running fine-tuning at different resolution than training is common practice (Kolesnikov\\net al., 2020)).\\n', 14, [296.7345886230469, 640.3866577148438, 1404.1080322265625, 760.5717163085938])\n",
      "('CONCLUSION\\n', 9, [335.4420166015625, 1082.6458740234375, 544.8648681640625, 1115.8797607421875])\n",
      "('EXPERIMENTS\\n', 4, [345.044677734375, 1417.9842529296875, 556.9210815429688, 1449.3145751953125])\n",
      "('RELATED WORK\\n', 2, [330.83380126953125, 628.6781616210938, 590.24072265625, 665.4181518554688])\n",
      "iou_top1: 0.8922150135040283\n",
      "3 [3, 14, 9, 4, 2]\n",
      "iou_top5: 0.8922150135040283\n",
      "\n",
      "Question: How is an image's patch embeddings calculated?\n",
      "('An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\\nsequence of token embeddings. To handle 2D images, we reshape the image x € R?*\"*C into a\\nsequence of flattened 2D patches x, € RN*x(P*-C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P?\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\\nflatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\\nthe output of this projection as the patch embeddings.\\n', 3, [292.84619140625, 1361.54638671875, 1399.117919921875, 1621.5])\n",
      "('Hybrid Architecture. As an alternative to raw image patches, the input sequence can be formed\\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\\nflattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\\nThe classification input embedding and position embeddings are added as described above.\\n', 4, [303.16436767578125, 793.3269653320312, 1398.170654296875, 971.5714111328125])\n",
      "('After the projection, a learned position embedding is added to the\\npatch representations. Figure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-\\nbeddings. Further, the row-column structure appears; patches in the\\nsame row/column have similar embeddings. Finally, a sinusoidal\\nstructure is sometimes apparent for larger grids (Appendix D). That\\nthe position embeddings learn to represent 2D image topology ex-\\nplains why hand-crafted 2D-aware embedding variants do not yield\\nimprovements (Appendix D.4).\\n', 8, [298.1349182128906, 1151.1461181640625, 1047.146484375, 1461.72216796875])\n",
      "('Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (20 = Xclass), Whose state at the output of the Transformer encoder (CAD) serves as the\\nimage representation y (Eq. 4). Both during pre-training and fine-tuning, a classification head is at-\\ntached to z?.. The classification head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at fine-tuning time.\\n', 3, [296.7657775878906, 1634.37548828125, 1401.5657958984375, 1788.539306640625])\n",
      "('Figure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classification, we use the standard approach of adding an extra learnable\\n“classification token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n', 3, [298.2052307128906, 732.6156616210938, 1400.2398681640625, 894.8108520507812])\n",
      "iou_top1: 0.9448276162147522\n",
      "3 [3, 4, 8, 3, 3]\n",
      "iou_top5: 0.9448276162147522\n",
      "\n",
      "Question: How is the class token used in the embeddings?\n",
      "('Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (20 = Xclass), Whose state at the output of the Transformer encoder (CAD) serves as the\\nimage representation y (Eq. 4). Both during pre-training and fine-tuning, a classification head is at-\\ntached to z?.. The classification head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at fine-tuning time.\\n', 3, [296.7657775878906, 1634.37548828125, 1401.5657958984375, 1788.539306640625])\n",
      "('This design is inherited from the Transformer model for text, and we use it throughout the mai\\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP\\nthem, followed by a linear classifier—just like ResNet’s final feature map—performed very poorly\\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead\\n', 16, [305.5595397949219, 1911.849853515625, 1386.0107421875, 2037.18603515625])\n",
      "('Figure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classification, we use the standard approach of adding an extra learnable\\n“classification token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n', 3, [298.2052307128906, 732.6156616210938, 1400.2398681640625, 894.8108520507812])\n",
      "('In order to stay as close as possible to the original Transformer model, we made use of an additional\\n\\n[class] token, which is taken as image representation. The output of this token is then trans-\\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\\nin the single hidden layer.\\n', 16, [303.8602294921875, 1772.33203125, 1400.0115966796875, 1899.2979736328125])\n",
      "('Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modifications. To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\\nthe model on image classification in supervised fashion.\\n', 1, [299.5771484375, 1684.71337890625, 1397.6695556640625, 1837.021728515625])\n",
      "iou_top1: 0.9528301954269409\n",
      "3 [3, 16, 3, 16, 1]\n",
      "iou_top5: 0.9528301954269409\n",
      "\n",
      "Question: How does the model encode positional information?\n",
      "('Table 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\\nthere is a large gap between the performances of the model with no positional embedding and mod-\\nels with positional embedding, there is little to no difference between different ways of encoding\\npositional information. We speculate that since our Transformer encoder operates on patch-level\\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\\npixel-level inputs, e.g., 14 x 14 as opposed to 224 x 224, and learning to represent the spatial re-\\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\\nthe specific pattern of position embedding similarity learned by the network depends on the training\\nhvpnernarameters (Ficure 10).\\n', 18, [302.4027099609375, 847.6278686523438, 1398.43603515625, 1143.49853515625])\n",
      "('In addition to different ways of encoding spatial information, we also tried different ways of in-\\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\\n', 17, [292.6195983886719, 1944.20751953125, 1409.8720703125, 2035.066162109375])\n",
      "('Figure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classification, we use the standard approach of adding an extra learnable\\n“classification token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n', 3, [298.2052307128906, 732.6156616210938, 1400.2398681640625, 894.8108520507812])\n",
      "('the stem of them model and before feeding the inputs to the Transformer encoder (default across\\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\\neach layer (shared between layers).\\n', 18, [292.2285461425781, 713.7245483398438, 1401.3438720703125, 838.6392211914062])\n",
      "('After the projection, a learned position embedding is added to the\\npatch representations. Figure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-\\nbeddings. Further, the row-column structure appears; patches in the\\nsame row/column have similar embeddings. Finally, a sinusoidal\\nstructure is sometimes apparent for larger grids (Appendix D). That\\nthe position embeddings learn to represent 2D image topology ex-\\nplains why hand-crafted 2D-aware embedding variants do not yield\\nimprovements (Appendix D.4).\\n', 8, [298.1349182128906, 1151.1461181640625, 1047.146484375, 1461.72216796875])\n",
      "3 [18, 17, 3, 18, 8]\n",
      "iou_top5: 0.0\n",
      "\n",
      "Question: What parameters were used to train and fine-tune ViT models?\n",
      "('First, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\\nparameters — weight decay, dropout, and label smoothing. Figure 3 shows the results after fine-\\ntuning to ImageNet (results on other datasets are shown in Table 5)’. When pre-trained on the\\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\\nwith JFT-300M, do we see the full benefit of larger models. Figure 3 also shows the performance\\n', 6, [300.9099426269531, 1737.0577392578125, 1403.4168701171875, 1958.9293212890625])\n",
      "('We fine-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\\nremaining data. For final results we train on the entire training set and evaluate on the respective\\ntest data. For fine-tuning ResNets and hybrid models we use the exact same setup, with the only\\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\\n', 13, [303.53271484375, 1849.8763427734375, 1402.0462646484375, 2037.11865234375])\n",
      "('Training & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\\n2015) with 6; = 0.9, B2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\\nrate warmup and decay, see Appendix B.1 for details. For fine-tuning we use SGD with momentum,\\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we fine-tuned at\\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\n', 5, [298.05810546875, 1077.140380859375, 1401.2149658203125, 1327.56005859375])\n",
      "('Typically, we pre-train ViT on large datasets, and fine-tune to (smaller) downstream tasks. For\\nthis, we remove the pre-trained prediction head and attach a zero-initialized D x K feedforward\\nlayer, where K is the number of downstream classes. It is often beneficial to fine-tune at higher\\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\\n2D interpolation of the pre-trained position embeddings, according to their location in the original\\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\\n', 4, [297.8885192871094, 1059.15576171875, 1407.7777099609375, 1373.30712890625])\n",
      "('We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n', 4, [299.4459533691406, 1479.2294921875, 1404.611572265625, 1669.601806640625])\n",
      "5 [6, 13, 5, 4, 4]\n",
      "iou_top5: 0.9501354694366455\n",
      "\n",
      "Question: What is the impact of pre-training dataset size on model performance?\n",
      "('First, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\\nparameters — weight decay, dropout, and label smoothing. Figure 3 shows the results after fine-\\ntuning to ImageNet (results on other datasets are shown in Table 5)’. When pre-trained on the\\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\\nwith JFT-300M, do we see the full benefit of larger models. Figure 3 also shows the performance\\n', 6, [300.9099426269531, 1737.0577392578125, 1403.4168701171875, 1958.9293212890625])\n",
      "('model still took substantially less compute to pre-train than prior state of the art. However, we note\\nthat pre-training efficiency may be affected not only by the architecture choice, but also other pa-\\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\\nproximately 30 days.\\n', 6, [305.191650390625, 1145.5047607421875, 1401.93115234375, 1362.623046875])\n",
      "('However, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nfind that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n', 2, [306.51556396484375, 308.9344787597656, 1405.1368408203125, 526.0833129882812])\n",
      "('We perform a controlled scaling study of different models by evaluating transfer performance from\\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\\nbackbone).\\n', 8, [295.8568420410156, 285.8457336425781, 1408.8583984375, 569.0543823242188])\n",
      "('Table 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\\non the more challenging datasets — ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\\n', 5, [298.77545166015625, 1913.5302734375, 1408.7840576171875, 2036.3819580078125])\n",
      "iou_top1: 0.0\n",
      "6 [6, 6, 2, 8, 5]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.0\n",
      "\n",
      "Question: How does ViT compare to ResNets in terms of computational efficiency?\n",
      "('Figure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size fitting on device with\\nvarious architectures across input sizes. ViT models are clearly more memory-efficient.\\n', 19, [304.94781494140625, 1097.8580322265625, 1394.908935546875, 1191.6317138671875])\n",
      "('Another quantity of interest is the largest batch-size each model can fit onto a core, larger being\\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\\nThis shows that large ViT models have a clear advantage in terms of memory-efficiency over ResNet\\n\\nmodels.\\n', 19, [291.882568359375, 447.5191650390625, 1401.911376953125, 573.2955932617188])\n",
      "('Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\\nperformance/compute trade-off. ViT uses approximately 2 — 4x less compute to attain the same\\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\\n', 8, [306.2226257324219, 577.0025024414062, 1398.74462890625, 831.510986328125])\n",
      "('Figure 3: Transfer to ImageNet. While\\nlarge ViT models perform worse than BiT\\nResNets (shaded area) when pre-trained on\\nsmall datasets, they shine when pre-trained on\\nlarger datasets. Similarly, larger ViT variants\\novertake smaller ones as the dataset grows.\\n', 7, [300.79559326171875, 628.646240234375, 817.439208984375, 807.6524658203125])\n",
      "('We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n', 4, [299.4459533691406, 1479.2294921875, 1404.611572265625, 1669.601806640625])\n",
      "8 [19, 19, 8, 7, 4]\n",
      "iou_top5: 0.9322115182876587\n",
      "\n",
      "Question: Where was the research conducted?\n",
      "('The work was performed in Berlin, Ziirich, and Amsterdam. We thank many colleagues at Google\\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\\nraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lu¢i¢, Noam\\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\\n', 9, [308.05126953125, 1674.242919921875, 1401.5521240234375, 1830.0443115234375])\n",
      "('EXPERIMENTS\\n', 4, [345.044677734375, 1417.9842529296875, 556.9210815429688, 1449.3145751953125])\n",
      "('3Our implementation is based on the open-sourced PyTorch implementation in https: //github.com/\\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\\nunlocked by a carefully optimized implementation.\\n', 19, [296.9746398925781, 1892.3829345703125, 1399.5670166015625, 2033.72314453125])\n",
      "('RELATED WORK\\n', 2, [330.83380126953125, 628.6781616210938, 590.24072265625, 665.4181518554688])\n",
      "('dan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In CVPR, 2018.\\n', 10, [312.3311767578125, 1686.9583740234375, 1393.7314453125, 1743.93212890625])\n",
      "iou_top1: 0.9247915744781494\n",
      "9 [9, 4, 19, 2, 10]\n",
      "iou_top5: 0.9247915744781494\n",
      "\n",
      "Question: What advantage does self-attention provide in the Vision Transformer?\n",
      "('We have explored the direct application of Transformers to image recognition. Unlike prior works\\nusing self-attention in computer vision, we do not introduce image-specific inductive biases into\\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classification\\ndatasets, whilst being relatively cheap to pre-train.\\n', 9, [300.0313415527344, 1146.4471435546875, 1412.639404296875, 1365.279052734375])\n",
      "('Inductive bias. We note that Vision Transformer has much less image-specific inductive bias than\\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat fine-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below). Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\n', 4, [297.589111328125, 476.5500793457031, 1403.140869140625, 749.5263061523438])\n",
      "('While the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classification tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.!\\n', 1, [394.4130859375, 746.5789184570312, 1302.3428955078125, 1086.92041015625])\n",
      "('Self-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Specifically, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive field size in CNNS.\\n', 8, [304.0452575683594, 1475.1290283203125, 1043.155029296875, 1648.894775390625])\n",
      "('Naive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefficiently on hardware accelerators.\\n', 2, [303.70391845703125, 890.9014892578125, 1410.692138671875, 1262.37744140625])\n",
      "8 [9, 4, 1, 8, 2]\n",
      "iou_top5: 0.9197155237197876\n",
      "\n",
      "Question: What function does the hybrid ViT architecture implement differently?\n",
      "('We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n', 4, [299.4459533691406, 1479.2294921875, 1404.611572265625, 1669.601806640625])\n",
      "('Figure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\\nperformance/compute trade-off. ViT uses approximately 2 — 4x less compute to attain the same\\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\\n', 8, [306.2226257324219, 577.0025024414062, 1398.74462890625, 831.510986328125])\n",
      "('Moreover, we have modified ViT to process inputs in the 2-dimensional shape, instead of a 1-\\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\\ncolumn-self-attention plus an MLP.\\n', 19, [301.7151184082031, 1606.1064453125, 1408.9730224609375, 1726.9171142578125])\n",
      "('Table 3 summarizes our training setups for our different models. We found strong regularization\\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\\ntraining is done on resolution 224.\\n', 13, [305.7555847167969, 1608.677978515625, 1401.6014404296875, 1769.235107421875])\n",
      "('Hybrid Architecture. As an alternative to raw image patches, the input sequence can be formed\\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\\nflattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\\nThe classification input embedding and position embeddings are added as described above.\\n', 4, [303.16436767578125, 793.3269653320312, 1398.170654296875, 971.5714111328125])\n",
      "iou_top1: 0.0\n",
      "4 [4, 8, 19, 13, 4]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.9538928270339966\n",
      "\n",
      "Question: What are the major findings of the ViT scaling study?\n",
      "('We ran ablations on scaling different dimensions of the Transformer architecture to find out which\\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\\nfor different configurations. All configurations are based on a ViT model with 8 layers, D = 1024,\\nDyrp = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\\neffective sequence length shows surprisingly robust improvements without introducing parameters.\\nThese findings suggest that compute might be a better predictor of performance than the number of\\nparameters, and that scaling should emphasize depth over width if any. Overall, we find that scaling\\nall dimensions proportionally results in robust improvements.\\n', 16, [305.186767578125, 1334.767578125, 1401.5804443359375, 1683.0609130859375])\n",
      "('While these initial results are encouraging, many challenges remain. One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\\n', 9, [299.9452819824219, 1378.47216796875, 1404.1336669921875, 1563.906982421875])\n",
      "('Figure 12 (left) shows how many images one core can handle per second, across various input sizes.\\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\\nfor the largest models at the largest resolutions.\\n', 19, [292.9806823730469, 311.1744384765625, 1399.4512939453125, 432.0323791503906])\n",
      "('However, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nfind that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n', 2, [306.51556396484375, 308.9344787597656, 1405.1368408203125, 526.0833129882812])\n",
      "('Another quantity of interest is the largest batch-size each model can fit onto a core, larger being\\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\\nThis shows that large ViT models have a clear advantage in terms of memory-efficiency over ResNet\\n\\nmodels.\\n', 19, [291.882568359375, 447.5191650390625, 1401.911376953125, 573.2955932617188])\n",
      "\n",
      "Question: How does the attention mechanism in ViT differ from convolutions?\n",
      "('While the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classification tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.!\\n', 1, [394.4130859375, 746.5789184570312, 1302.3428955078125, 1086.92041015625])\n",
      "('To understand how ViT uses self-attention to integrate information across the image, we analyzed\\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\\ndistance” is analogous to receptive field size in CNNs. Average attention distance is highly variable\\nacross heads in lower layers, with some heads attending to much of the image, while others attend\\nto small regions at or near the query location. As depth increases, attention distance increases for all\\nheads. In the second half of the network, most heads attend widely across tokens.\\n', 20, [302.955810546875, 1092.0433349609375, 1400.27490234375, 1277.8309326171875])\n",
      "('Moreover, we have modified ViT to process inputs in the 2-dimensional shape, instead of a 1-\\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\\ncolumn-self-attention plus an MLP.\\n', 19, [301.7151184082031, 1606.1064453125, 1408.9730224609375, 1726.9171142578125])\n",
      "('Self-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Specifically, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive field size in CNNS.\\n', 8, [304.0452575683594, 1475.1290283203125, 1043.155029296875, 1648.894775390625])\n",
      "('Inductive bias. We note that Vision Transformer has much less image-specific inductive bias than\\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat fine-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below). Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\n', 4, [297.589111328125, 476.5500793457031, 1403.140869140625, 749.5263061523438])\n",
      "8 [1, 20, 19, 8, 4]\n",
      "iou_top5: 0.0\n",
      "\n",
      "Question: How does the self-supervised training experiment influence ViT performance?\n",
      "('While these initial results are encouraging, many challenges remain. One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\\n', 9, [299.9452819824219, 1378.47216796875, 1404.1336669921875, 1563.906982421875])\n",
      "('We evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n', 4, [299.4459533691406, 1479.2294921875, 1404.611572265625, 1669.601806640625])\n",
      "('et al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\\nsignificant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\\net al., 2020b; He et al., 2020; Bachman et al., 2019; Hénaff et al., 2020) to future work.\\n', 9, [301.0882568359375, 850.4146118164062, 1402.399169921875, 1039.9256591796875])\n",
      "('Self-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Specifically, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive field size in CNNS.\\n', 8, [304.0452575683594, 1475.1290283203125, 1043.155029296875, 1648.894775390625])\n",
      "('However, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nfind that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n', 2, [306.51556396484375, 308.9344787597656, 1405.1368408203125, 526.0833129882812])\n",
      "iou_top1: 0.0\n",
      "9 [9, 4, 9, 8, 2]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.9333650469779968\n",
      "\n",
      "Question: How does the receptive field of ViT change across layers?\n",
      "('To understand how ViT uses self-attention to integrate information across the image, we analyzed\\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\\ndistance” is analogous to receptive field size in CNNs. Average attention distance is highly variable\\nacross heads in lower layers, with some heads attending to much of the image, while others attend\\nto small regions at or near the query location. As depth increases, attention distance increases for all\\nheads. In the second half of the network, most heads attend widely across tokens.\\n', 20, [302.955810546875, 1092.0433349609375, 1400.27490234375, 1277.8309326171875])\n",
      "('Self-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Specifically, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive field size in CNNS.\\n', 8, [304.0452575683594, 1475.1290283203125, 1043.155029296875, 1648.894775390625])\n",
      "('To compute maps of the attention from the output token to the input space (Figures 6 and 14), we\\nused Attention Rollout (Abnar & Zuidema, 2020). Briefly, we averaged attention weights of ViT-\\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This account:\\nfor the mixing of attention across tokens through all layers.\\n', 20, [298.8843994140625, 1369.7122802734375, 1390.1365966796875, 1496.9210205078125])\n",
      "('Inductive bias. We note that Vision Transformer has much less image-specific inductive bias than\\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat fine-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below). Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\n', 4, [297.589111328125, 476.5500793457031, 1403.140869140625, 749.5263061523438])\n",
      "('When transferring ViT models to another dataset, we remove the whole head (two linear layers) and\\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\\n', 14, [297.41485595703125, 776.80712890625, 1430.32421875, 870.9312133789062])\n",
      "iou_top1: 0.9505819082260132\n",
      "20 [20, 8, 20, 4, 14]\n",
      "iou_top5: 0.9505819082260132\n",
      "\n",
      "Question: What results did ViT-H/14 achieve on the ImageNet benchmark?\n",
      "('We also evaluate our flagship ViT-H/14 model on the ObjectNet benchmark following the evaluation\\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\\n', 20, [295.97882080078125, 1588.3675537109375, 1409.518310546875, 1652.984130859375])\n",
      "('However, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nfind that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n', 2, [306.51556396484375, 308.9344787597656, 1405.1368408203125, 526.0833129882812])\n",
      "('Figure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\\nmethods on this benchmark: BiT, VIVI—a ResNet co-trained on ImageNet and Youtube (Tschannen\\net al., 2020), and S4L — supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\\nSpecialized the performance of the top two models is similar.\\n', 6, [305.6623229980469, 1380.654052734375, 1408.5546875, 1535.2694091796875])\n",
      "('Table 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\\non the more challenging datasets — ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\\n', 5, [298.77545166015625, 1913.5302734375, 1408.7840576171875, 2036.3819580078125])\n",
      "('et al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\\nsignificant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\\net al., 2020b; He et al., 2020; Bachman et al., 2019; Hénaff et al., 2020) to future work.\\n', 9, [301.0882568359375, 850.4146118164062, 1402.399169921875, 1039.9256591796875])\n",
      "6 [20, 2, 6, 5, 9]\n",
      "iou_top5: 0.0\n",
      "\n",
      "Question: Why is it not recommended to apply self attention directly to images?\n",
      "('Naive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefficiently on hardware accelerators.\\n', 2, [303.70391845703125, 890.9014892578125, 1410.692138671875, 1262.37744140625])\n",
      "('Naive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefficiently on hardware accelerators.\\n', 2, [303.70391845703125, 890.9014892578125, 1410.692138671875, 1262.37744140625])\n",
      "('We have explored the direct application of Transformers to image recognition. Unlike prior works\\nusing self-attention in computer vision, we do not introduce image-specific inductive biases into\\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classification\\ndatasets, whilst being relatively cheap to pre-train.\\n', 9, [300.0313415527344, 1146.4471435546875, 1412.639404296875, 1365.279052734375])\n",
      "('Self-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Specifically, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive field size in CNNS.\\n', 8, [304.0452575683594, 1475.1290283203125, 1043.155029296875, 1648.894775390625])\n",
      "('While the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classification tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.!\\n', 1, [394.4130859375, 746.5789184570312, 1302.3428955078125, 1086.92041015625])\n",
      "iou_top1: 0.9406457543373108\n",
      "2 [2, 2, 9, 8, 1]\n",
      "iou_top5: 0.9406457543373108\n",
      "\n",
      "Question: What future areas of research do the authors suggest?\n",
      "('RELATED WORK\\n', 2, [330.83380126953125, 628.6781616210938, 590.24072265625, 665.4181518554688])\n",
      "('CONCLUSION\\n', 9, [335.4420166015625, 1082.6458740234375, 544.8648681640625, 1115.8797607421875])\n",
      "('Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’ Amour, Dan Moldovan, Sylvan\\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\\nlutional neural networks. arXiv, 2020.\\n', 10, [305.1754455566406, 1285.896240234375, 1398.170166015625, 1411.9967041015625])\n",
      "('While these initial results are encouraging, many challenges remain. One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\\n', 9, [299.9452819824219, 1378.47216796875, 1404.1336669921875, 1563.906982421875])\n",
      "('-an-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\\nattention and convolutional layers. In JCLR, 2020.\\n', 10, [321.0652770996094, 1030.058837890625, 1395.1441650390625, 1089.197509765625])\n",
      "9 [2, 9, 10, 9, 10]\n",
      "iou_top5: 0.0\n",
      "iou_top5: 0.9528160691261292\n",
      "Top-1\n",
      "Correct pages: 0.55\n",
      "Correct regions: 0.35, IoU: 0.9395497441291809\n",
      "Top-5\n",
      "Correct pages: 0.95\n",
      "Correct regions: 0.7, IoU: 0.9416966438293457\n",
      "\n",
      "\n",
      "\n",
      "Top-1\n",
      "Correct pages: 0.7666666666666667\n",
      "Correct regions: 0.6333333333333333, IoU: 0.9433769583702087\n",
      "Top-5\n",
      "Correct pages: 0.95\n",
      "Correct regions: 0.8333333333333334, IoU: 0.9454734921455383\n"
     ]
    }
   ],
   "source": [
    "# Experiment section: Q-A over 3 documents\n",
    "from document_analysis import DocumentAnalysis\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Helper functions\n",
    "# Formatting Q-A pairs with COCO annotations\n",
    "# Formula for IoU\n",
    "def calculate_iou(box1, box2):\n",
    "    box1 = np.array(box1, dtype=np.float32)\n",
    "    box2 = np.array(box2, dtype=np.float32)\n",
    "\n",
    "    # Compute intersection coordinates\n",
    "    inter_x_min = np.maximum(box1[0], box2[0])\n",
    "    inter_y_min = np.maximum(box1[1], box2[1])\n",
    "    inter_x_max = np.minimum(box1[2], box2[2])\n",
    "    inter_y_max = np.minimum(box1[3], box2[3])\n",
    "\n",
    "    # Compute intersection area\n",
    "    inter_width = np.maximum(0, inter_x_max - inter_x_min)\n",
    "    inter_height = np.maximum(0, inter_y_max - inter_y_min)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    # Compute area of both boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # Compute union area with numerical stability\n",
    "    union_area = np.maximum(box1_area + box2_area - inter_area, 1e-10)\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = inter_area / union_area\n",
    "    return np.clip(iou, 0.0, 1.0)\n",
    "\n",
    "# Convert (x, y, width, height) to (x1, y1, x2, y2)\n",
    "def coco_to_coordinates(bbox):\n",
    "    x, y, width, height = bbox # unwrap\n",
    "    return [x, y, x + width, y + height]\n",
    "\n",
    "\n",
    "# Filename init\n",
    "data_dir = \"../data/\"\n",
    "data_annotations_dir = \"../data/annotations/\"\n",
    "doc_list = [\"1706.03762.pdf\", \"imagenet-classification.pdf\", \"2010.11929v2.pdf\"]\n",
    "annotations_list = [\"annotations-attention\", \"annotations-imagenet\", \"annotations-vit\"]\n",
    "qa_pairs_list = [\"qa-pairs-attention\", \"qa-pairs-imagenet\", \"qa-pairs-vit\"]\n",
    "\n",
    "# Metrics\n",
    "# Total pages, questions and total regions fixed at 20*3 = 60\n",
    "correct_pages_top1 = 0\n",
    "correct_regions_top1 = 0\n",
    "correct_regions_iou_top1 = 0.0\n",
    "correct_pages_top5 = 0\n",
    "correct_regions_top5 = 0\n",
    "correct_regions_iou_top5 = 0.0\n",
    "\n",
    "for doc_name, annotations_name, qa_pairs_name in zip(doc_list, annotations_list, qa_pairs_list):\n",
    "    # if doc_name == \"1706.03762.pdf\": continue\n",
    "    print(f\"\\n\\nCurrent doc: {doc_name}\")\n",
    "    annotations = json.load(open(data_annotations_dir + annotations_name + '.json', 'r'))\n",
    "    qa_pairs = json.load(open(data_annotations_dir + qa_pairs_name + '.json', 'r'))\n",
    "\n",
    "    # New pipeline, read and process \n",
    "    pipeline = DocumentAnalysis(vector_dir = '../data/.vectorstore/')\n",
    "    # doc = pipeline.read_from_path(data_dir + doc_name)\n",
    "    # pipeline.process_document(doc)\n",
    "    # pipeline.faiss_persist(subdir = doc_name + '/') # one-time, update schema\n",
    "    pipeline.faiss_read(subdir = doc_name + '/') # If document has been processed and stored prior\n",
    "\n",
    "    # Metrics for current paper\n",
    "    cpt1_trial = 0\n",
    "    crt1_trial = 0\n",
    "    crtiou1_trial = 0.0\n",
    "    cpt5_trial = 0\n",
    "    crt5_trial = 0\n",
    "    crtiou5_trial = 0.0\n",
    "\n",
    "    # Q-A assessment\n",
    "    verbose=True\n",
    "    for qa in qa_pairs:\n",
    "        qvalue, qpage, qbbox = qa['answer'].values()\n",
    "        qbbox = tuple(coco_to_coordinates(qbbox)) # Standardize to LayoutParser bbox system\n",
    "\n",
    "        answers = pipeline.search_faiss(qa['question'])\n",
    "        atext = [a['content'] for a in answers]\n",
    "        apages = [a['page']+1 for a in answers] # Add 1 to convert from index to numbering\n",
    "        abboxes = [a['bbox'] for a in answers]\n",
    "\n",
    "        # Verbose illustration\n",
    "        if verbose:\n",
    "            print(f'\\nQuestion: {qa['question']}')\n",
    "            # print(f'\\nground: {qpage}, {qbbox}, {qa['question']}')\n",
    "            for i in zip(atext, apages, abboxes):\n",
    "                print(i)\n",
    "\n",
    "        # Top-1 metric\n",
    "        # apages is ordered in decreasing order\n",
    "        if apages[0] == qpage:\n",
    "            cpt1_trial += 1\n",
    "            iou = calculate_iou(qbbox, abboxes[0])\n",
    "            if verbose: print(f'iou_top1: {iou}')\n",
    "            if iou > 0.5: \n",
    "                crt1_trial += 1\n",
    "                crtiou1_trial += iou\n",
    "        \n",
    "        # Top-5 metric\n",
    "        if qpage in apages:\n",
    "            if verbose: print(qpage, apages)\n",
    "            cpt5_trial += 1\n",
    "            for apage, abbox in zip(apages, abboxes):\n",
    "                if apage == qpage:\n",
    "                    iou = calculate_iou(qbbox, abbox)\n",
    "                    if verbose: print(f'iou_top5: {iou}')\n",
    "                    if iou > 0.5: \n",
    "                        crt5_trial += 1\n",
    "                        crtiou5_trial += iou\n",
    "                        break # If correct found, skip remaining chunks\n",
    "    # Print document-specific metrics\n",
    "    qa_length = len(qa_pairs)\n",
    "    print(\"Top-1\")\n",
    "    print(f'Correct pages: {cpt1_trial/qa_length}')\n",
    "    print(f'Correct regions: {crt1_trial/qa_length}, IoU: {crtiou1_trial/crt1_trial}')\n",
    "    print(\"Top-5\")\n",
    "    print(f\"Correct pages: {cpt5_trial/qa_length}\")\n",
    "    print(f'Correct regions: {crt5_trial/qa_length}, IoU: {crtiou5_trial/crt5_trial}')\n",
    "    print('\\n\\n')\n",
    "\n",
    "    # Aggregate\n",
    "    correct_pages_top1 += cpt1_trial\n",
    "    correct_regions_top1 += crt1_trial\n",
    "    correct_regions_iou_top1 += crtiou1_trial\n",
    "    correct_pages_top5 += cpt5_trial\n",
    "    correct_regions_top5 += crt5_trial\n",
    "    correct_regions_iou_top5 += crtiou5_trial\n",
    "\n",
    "# Print overall metrics\n",
    "print(\"Top-1\")\n",
    "print(f'Correct pages: {correct_pages_top1/60}')\n",
    "print(f'Correct regions: {correct_regions_top1/60}, IoU: {correct_regions_iou_top1/correct_regions_top1}')\n",
    "print(\"Top-5\")\n",
    "print(f\"Correct pages: {correct_pages_top5/60}\")\n",
    "print(f'Correct regions: {correct_regions_top5/60}, IoU: {correct_regions_iou_top5/correct_regions_top5}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
