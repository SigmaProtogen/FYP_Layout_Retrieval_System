{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lewis\\OneDrive\\Desktop\\Schoolwork\\FYP\\FYProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import layoutparser as lp\n",
    "import pymupdf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPConfig, CLIPTokenizer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified class\n",
    "class DocumentAnalysis():\n",
    "    def __init__(self, embedding_model = \"openai/clip-vit-base-patch32\", cross_encoder_model = \"cross-encoder/ms-marco-MiniLM-L6-v2\"):\n",
    "        # Layout detection\n",
    "        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_R_50_FPN_3x/config', \n",
    "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n",
    "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n",
    "        self.ocr_agent = lp.TesseractAgent(languages='eng') \n",
    "\n",
    "        # Dual encoders for embeddings\n",
    "        self.clip_model = CLIPModel.from_pretrained(embedding_model)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(embedding_model)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(embedding_model)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(self.tokenizer, chunk_size=77, chunk_overlap=5)\n",
    "\n",
    "        # Cross encoder for retrieval-reranking\n",
    "        self.cross_encoder_tokenizer = AutoTokenizer.from_pretrained(cross_encoder_model)\n",
    "        self.cross_encoder = AutoModelForSequenceClassification.from_pretrained(cross_encoder_model)        \n",
    "\n",
    "        # Vectorstore variables\n",
    "        self.dimension = 512  # CLIP's embedding size\n",
    "        self.faiss_index = faiss.IndexFlatL2(self.dimension) # FAISS Vector store\n",
    "        self.metadata_store = {}  # Store mapping of IDs and document page number to content\n",
    "        self.vector_dir = '../data/.vectorstore/' # Directory to write data to\n",
    "\n",
    "    # Read a PDF document using PyMuPDF\n",
    "    # Returns list of page images in cv2 format\n",
    "    def read_from_path(self, filepath):\n",
    "        doc = pymupdf.open(filepath)\n",
    "        return [self.pixmap_to_cv2(page.get_pixmap(dpi=300)) for page in doc]\n",
    "\n",
    "    # Convert PyMuPDF pixmap to cv2\n",
    "    def pixmap_to_cv2(self, pixmap):\n",
    "        bytes = np.frombuffer(pixmap.samples, dtype=np.uint8)\n",
    "        image = bytes.reshape(pixmap.height, pixmap.width, pixmap.n)\n",
    "        image = image[..., ::-1]\n",
    "        return image\n",
    "\n",
    "    # Takes in image object from read_from_path()\n",
    "    # Detects layout -> Processes ROI by label\n",
    "    def detect_layout(self, image):\n",
    "        layout = self.model.detect(image)\n",
    "\n",
    "        # Separate boxes by category\n",
    "        text_blocks = lp.Layout([b for b in layout if b.type=='Text'])\n",
    "        title_blocks = lp.Layout([b for b in layout if b.type=='Title'])\n",
    "        list_blocks = lp.Layout([b for b in layout if b.type=='List'])\n",
    "        table_blocks = lp.Layout([b for b in layout if b.type=='Table'])\n",
    "        figure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])\n",
    "\n",
    "        # Processing text blocks\n",
    "        # Eliminate text blocks nested in images/figures\n",
    "        text_blocks = lp.Layout([b for b in text_blocks \\\n",
    "                        if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "        # Sort boxes\n",
    "        h, w = image.shape[:2]\n",
    "        left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
    "        left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
    "        left_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "        # The b.coordinates[1] corresponds to the y coordinate of the region\n",
    "        # sort based on that can simulate the top-to-bottom reading order \n",
    "        right_blocks = lp.Layout([b for b in text_blocks if b not in left_blocks])\n",
    "        right_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "\n",
    "        # And finally combine the two lists and add the index\n",
    "        text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
    "\n",
    "        # Perform OCR to extract text\n",
    "        for block in text_blocks + title_blocks + list_blocks + table_blocks:\n",
    "            # Add padding in each image segment to improve robustness\n",
    "            text = self._ocr_on_block(image, block)\n",
    "            block.set(text=text, inplace=True) # Assign parsed text to block element\n",
    "            \n",
    "        # Return all blocks on the page as a list\n",
    "        # Omit titles as it affects retrieval\n",
    "        return text_blocks + list_blocks + table_blocks + figure_blocks\n",
    "\n",
    "    # Function to crop an image given block's bbox and additional padding\n",
    "    def _crop_image(self, image, block, padding=10):\n",
    "        return (block.pad(left=padding, right=padding, top=padding, bottom=padding).crop_image(image))\n",
    "\n",
    "    # Perform OCR to extract text given image and block (for text, tables and lists)\n",
    "    def _ocr_on_block(self, image, block):\n",
    "        # Add padding in each image segment to improve robustness\n",
    "        segment_image = (block.pad(left=5, right=5, top=5, bottom=5).crop_image(image))\n",
    "        return self.ocr_agent.detect(segment_image)\n",
    "\n",
    "    # Vectorstore functions\n",
    "    # Function to chunk text to CLIP max length\n",
    "    def chunk_text(self, text):\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        return chunks\n",
    "\n",
    "    # Function to encode text\n",
    "    def encode_text(self, text):\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_text_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to encode image\n",
    "    def encode_image(self, image):\n",
    "        inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_image_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to add item to FAISS\n",
    "    # Specify content, type, page and bounding box from blocks\n",
    "    def add_to_faiss(self, embedding, content, content_type, page_idx, bbox):\n",
    "        idx = len(self.metadata_store)  # Assign unique index\n",
    "        self.faiss_index.add(embedding)\n",
    "        self.metadata_store[idx] = {\"type\": content_type, \"content\": content, \"page\": page_idx, \"bbox\": bbox}\n",
    "    \n",
    "    # Perform retrieval (and rerank)\n",
    "    def search_faiss(self, query, k=10):\n",
    "        query_embedding = self.encode_text(query)\n",
    "        _, indices = self.faiss_index.search(query_embedding, k)\n",
    "        # Convert to int (faiss_read may change it to numpy)\n",
    "        indices = [int(i) for i in indices[0]]\n",
    "        \n",
    "        # Display retrieved items\n",
    "        # retrieved items accessed by metadata_store using fetched indices\n",
    "        for idx in indices:\n",
    "            print(f\"Retrieved {self.metadata_store[idx]['type']}: {self.metadata_store[idx]['content']}\")\n",
    "        \n",
    "        # Cross encoder reranking on text modality\n",
    "        answers = [self.metadata_store[idx] for idx in indices if self.metadata_store[idx]['type']!='Figure']\n",
    "        answer_texts = [a['content'] for a in answers]\n",
    "        queries = [query for i in range(len(answers))] # Repeat for tokenizer input\n",
    "        features = self.cross_encoder_tokenizer(queries, answer_texts,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad(): # Rerank\n",
    "            scores = self.cross_encoder(**features).logits\n",
    "\n",
    "        # Select index with best score\n",
    "        best_index = np.argmax(scores)\n",
    "        best_answer = answers[best_index] # Answer with full metadata for downstream\n",
    "\n",
    "        # Display for debug\n",
    "        print(scores, best_answer)\n",
    "\n",
    "\n",
    "    # Writes the vectorstore and metadata into a given path\n",
    "    def faiss_persist(self):\n",
    "        faiss.write_index(self.faiss_index, self.vector_dir+\"faiss.index\")\n",
    "        json.dump(self.metadata_store, open(self.vector_dir+\"metadata.json\", 'w'))\n",
    "    \n",
    "    # Read from existing vector stores\n",
    "    def faiss_read(self):\n",
    "        self.faiss_index = faiss.read_index(self.vector_dir+\"faiss.index\")\n",
    "        self.metadata_store = json.load(open(self.vector_dir+\"metadata.json\", 'r'), object_hook=self._convert_keys)\n",
    "    \n",
    "    # Convert keys from string to int when deserializing\n",
    "    def _convert_keys(self, d):\n",
    "        return {int(k) if k.isdigit() else k: v for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Sample run for 1 document\n",
    "# For debug, run pipeline.faiss_read() in cell below to prevent rereading doc\n",
    "pipeline = DocumentAnalysis()\n",
    "doc_path = \"../data/1706.03762.pdf\"\n",
    "doc = pipeline.read_from_path(doc_path)\n",
    "\n",
    "# Processing for each page\n",
    "# Remove [:3] for entire doc, keep for testing\n",
    "for page_idx, page in enumerate(doc[:3]):\n",
    "    blocks = pipeline.detect_layout(page)\n",
    "\n",
    "    # Processing for each block to be vectorized\n",
    "    for b in blocks:\n",
    "        # Process as an image if detected type is a figure, else process as text\n",
    "        if b.type == \"Figure\":\n",
    "            # Crop and get image embeddings\n",
    "            segmented_image = pipeline._crop_image(page, b, padding=20)\n",
    "            image_embs = pipeline.encode_image(segmented_image)\n",
    "            pipeline.add_to_faiss(embedding=image_embs, content=\"Figure\", content_type=b.type, page_idx=page_idx, bbox=b.block.coordinates)\n",
    "        else:\n",
    "            # Chunk text and create new blocks, and process for each block\n",
    "            # Returns list even if not chunked\n",
    "            chunks = pipeline.chunk_text(b.text)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                b2 = deepcopy(b)\n",
    "                b2.set(text=chunk, inplace=True)\n",
    "\n",
    "                # Create duplicate blocks for each chunk\n",
    "                # Encode using text and add to FAISS\n",
    "                text_embs = pipeline.encode_text(b2.text)\n",
    "                pipeline.add_to_faiss(embedding=text_embs, content=b2.text, content_type=b.type, page_idx=page_idx, bbox=b2.block.coordinates)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = DocumentAnalysis()\n",
    "# pipeline.faiss_persist()\n",
    "# pipeline.faiss_read()\n",
    "query=\"How is the encoder used in the Transformer architecture?\"\n",
    "pipeline.search_faiss(query)\n",
    "pipeline.faiss_persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Text: The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "Retrieved Text: To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "Retrieved Text: In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "Retrieved Text: efforts have since continued to push the boundaries of recurrent language models and encoder-decodet\n",
      "architectures [38, 24, 15].\n",
      "Retrieved Text: Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "Retrieved Text: attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "Retrieved Text: Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "Retrieved Text: End-to-end memory networks are based on a recurrent attention mechanism instead of sequence\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "Retrieved Text: translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "Retrieved Text: based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "tensor([[  2.9878],\n",
      "        [ -0.9264],\n",
      "        [ -1.7196],\n",
      "        [ -2.2005],\n",
      "        [  2.0329],\n",
      "        [  1.6721],\n",
      "        [ -0.8547],\n",
      "        [-10.9888],\n",
      "        [-11.3517],\n",
      "        [-11.2720]]) {'type': 'Text', 'content': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,', 'page': 0, 'bbox': [598.13232421875, 1704.216064453125, 1954.327392578125, 2412.140625]}\n"
     ]
    }
   ],
   "source": [
    "pipeline = DocumentAnalysis()\n",
    "pipeline.faiss_read()\n",
    "query=\"How is the encoder used in the Transformer architecture?\"\n",
    "pipeline.search_faiss(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform retrieval\n",
    "# To implement 2 stage retrieval\n",
    "\n",
    "\n",
    "def search_faiss(query, k=3):\n",
    "    query_embedding = encode_text(query)\n",
    "    _, indices = faiss_index.search(query_embedding, k)\n",
    "    \n",
    "    # Display retrieved items\n",
    "    for idx in indices[0]:\n",
    "        print(f\"Retrieved {metadata_store[idx]['type']}: {metadata_store[idx]['content']}\")\n",
    "\n",
    "    # Cross encoder reranking on text modality\n",
    "    answers = [metadata_store[idx]['content'] for idx in indices[0] if metadata_store[idx]['type']!='Figure']\n",
    "    pairs = [(query, ans) for ans in answers]\n",
    "    scores = model.predict(pairs) # Predict\n",
    "\n",
    "    # Output should be ranked in descending order, so pick top\n",
    "    best_index = np.argmax(scores)\n",
    "    best_answer = answers[best_index]\n",
    "\n",
    "index = faiss.IndexFlatL2(512) # FAISS Vector store\n",
    "metadata = {}\n",
    "\n",
    "query = 'What is the population of Berlin?'\n",
    "anss ['Berlin has a population of 19 million people.', 'berlin is a nice place to live', 'among us']\n",
    "\n",
    "inputs = self.clip_processor(text=[text], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embedding = self.clip_model.get_text_features(**inputs).numpy()\n",
    "return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example: Retrieve images using text query\n",
    "search_faiss(\"solar energy\", query_type=\"text\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
