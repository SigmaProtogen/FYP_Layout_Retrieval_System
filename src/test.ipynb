{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lewis\\OneDrive\\Desktop\\Schoolwork\\FYP\\FYProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import layoutparser as lp\n",
    "import pymupdf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPConfig, CLIPTokenizer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified class\n",
    "class DocumentAnalysis():\n",
    "    def __init__(self, embedding_model = \"openai/clip-vit-base-patch32\", cross_encoder_model = \"cross-encoder/ms-marco-MiniLM-L6-v2\"):\n",
    "        # Layout detection\n",
    "        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_R_50_FPN_3x/config', \n",
    "                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n",
    "                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n",
    "        self.ocr_agent = lp.TesseractAgent(languages='eng') \n",
    "\n",
    "        # Dual encoders for embeddings\n",
    "        self.clip_model = CLIPModel.from_pretrained(embedding_model)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(embedding_model)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(embedding_model)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(self.tokenizer, chunk_size=77, chunk_overlap=5)\n",
    "\n",
    "        # Cross encoder for retrieval-reranking\n",
    "        self.cross_encoder_tokenizer = AutoTokenizer.from_pretrained(cross_encoder_model)\n",
    "        self.cross_encoder = AutoModelForSequenceClassification.from_pretrained(cross_encoder_model)        \n",
    "\n",
    "        # Vectorstore variables\n",
    "        self.dimension = 512  # CLIP's embedding size\n",
    "        self.faiss_index = faiss.IndexFlatL2(self.dimension) # FAISS Vector store\n",
    "        self.metadata_store = {}  # Store mapping of IDs and document page number to content\n",
    "        self.vector_dir = '../data/.vectorstore/' # Directory to write data to\n",
    "\n",
    "    # Read a PDF document using PyMuPDF\n",
    "    # Returns list of page images in cv2 format\n",
    "    def read_from_path(self, filepath):\n",
    "        doc = pymupdf.open(filepath)\n",
    "        return [self.pixmap_to_cv2(page.get_pixmap(dpi=300)) for page in doc]\n",
    "\n",
    "    # Convert PyMuPDF pixmap to cv2\n",
    "    def pixmap_to_cv2(self, pixmap):\n",
    "        bytes = np.frombuffer(pixmap.samples, dtype=np.uint8)\n",
    "        image = bytes.reshape(pixmap.height, pixmap.width, pixmap.n)\n",
    "        image = image[..., ::-1]\n",
    "        return image\n",
    "\n",
    "    # Takes in image object from read_from_path()\n",
    "    # Detects layout -> Processes ROI by label\n",
    "    def detect_layout(self, image):\n",
    "        layout = self.model.detect(image)\n",
    "\n",
    "        # Separate boxes by category\n",
    "        text_blocks = lp.Layout([b for b in layout if b.type=='Text'])\n",
    "        title_blocks = lp.Layout([b for b in layout if b.type=='Title'])\n",
    "        list_blocks = lp.Layout([b for b in layout if b.type=='List'])\n",
    "        table_blocks = lp.Layout([b for b in layout if b.type=='Table'])\n",
    "        figure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])\n",
    "\n",
    "        # Processing text blocks\n",
    "        # Eliminate text blocks nested in images/figures\n",
    "        text_blocks = lp.Layout([b for b in text_blocks \\\n",
    "                        if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "        # Sort boxes\n",
    "        h, w = image.shape[:2]\n",
    "        left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
    "        left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
    "        left_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "        # The b.coordinates[1] corresponds to the y coordinate of the region\n",
    "        # sort based on that can simulate the top-to-bottom reading order \n",
    "        right_blocks = lp.Layout([b for b in text_blocks if b not in left_blocks])\n",
    "        right_blocks.sort(key = lambda b:b.coordinates[1], inplace=True)\n",
    "\n",
    "        # And finally combine the two lists and add the index\n",
    "        text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
    "\n",
    "        # Perform OCR to extract text\n",
    "        for block in text_blocks + title_blocks + list_blocks + table_blocks:\n",
    "            # Add padding in each image segment to improve robustness\n",
    "            text = self._ocr_on_block(image, block)\n",
    "            block.set(text=text, inplace=True) # Assign parsed text to block element\n",
    "            \n",
    "        # Return all blocks on the page as a list\n",
    "        # Omit titles as it affects retrieval\n",
    "        return text_blocks + list_blocks + table_blocks + figure_blocks\n",
    "\n",
    "    # Function to crop an image given block's bbox and additional padding\n",
    "    def _crop_image(self, image, block, padding=10):\n",
    "        return (block.pad(left=padding, right=padding, top=padding, bottom=padding).crop_image(image))\n",
    "\n",
    "    # Perform OCR to extract text given image and block (for text, tables and lists)\n",
    "    def _ocr_on_block(self, image, block):\n",
    "        # Add padding in each image segment to improve robustness\n",
    "        segment_image = (block.pad(left=5, right=5, top=5, bottom=5).crop_image(image))\n",
    "        return self.ocr_agent.detect(segment_image)\n",
    "\n",
    "    # Vectorstore functions\n",
    "    # Function to chunk text to CLIP max length\n",
    "    def chunk_text(self, text):\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        return chunks\n",
    "\n",
    "    # Function to encode text\n",
    "    def encode_text(self, text):\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_text_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to encode image\n",
    "    def encode_image(self, image):\n",
    "        inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            embedding = self.clip_model.get_image_features(**inputs).numpy()\n",
    "        return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "    # Function to add item to FAISS\n",
    "    # Specify content, type, page and bounding box from blocks\n",
    "    def add_to_faiss(self, embedding, content, content_type, page_idx, bbox):\n",
    "        idx = len(self.metadata_store)  # Assign unique index\n",
    "        self.faiss_index.add(embedding)\n",
    "        self.metadata_store[idx] = {\"type\": content_type, \"content\": content, \"page\": page_idx, \"bbox\": bbox}\n",
    "    \n",
    "    # Perform retrieval (and rerank)\n",
    "    def search_faiss(self, query, k=10):\n",
    "        query_embedding = self.encode_text(query)\n",
    "        _, indices = self.faiss_index.search(query_embedding, k)\n",
    "        # Convert to int\n",
    "        indices = [int(i) for i in indices]\n",
    "        \n",
    "        # Display retrieved items\n",
    "        # retrieved items accessed by metadata_store using fetched indices\n",
    "        for idx in indices[0]:\n",
    "            print(f\"Retrieved {self.metadata_store[idx]['type']}: {self.metadata_store[idx]['content']}\")\n",
    "        \n",
    "        # Cross encoder reranking on text modality\n",
    "        answers = [self.metadata_store[idx] for idx in indices[0] if self.metadata_store[idx]['type']!='Figure']\n",
    "        queries = [query for i in range(len(answers))] # Repeat for tokenizer input\n",
    "        features = tokenizer(queries, answers,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad(): # Rerank\n",
    "            scores = model(**features).logits\n",
    "\n",
    "        # Select index with best score\n",
    "        best_index = np.argmax(scores)\n",
    "        best_answer = answers[best_index] # Answer with full metadata for downstream\n",
    "\n",
    "        # Display for debug\n",
    "        print(best_answer)\n",
    "\n",
    "\n",
    "    # Writes the vectorstore and metadata into a given path\n",
    "    def faiss_persist(self):\n",
    "        faiss.write_index(self.faiss_index, self.vector_dir+\"faiss.index\")\n",
    "        json.dump(self.metadata_store, open(self.vector_dir+\"metadata.json\", 'w'))\n",
    "    \n",
    "    # Read from existing vector stores\n",
    "    def faiss_read(self):\n",
    "        self.faiss_index = faiss.read_index(self.vector_dir+\"faiss.index\")\n",
    "        self.metadata_store = json.load(open(self.vector_dir+\"metadata.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lewis\\OneDrive\\Desktop\\Schoolwork\\FYP\\FYProject\\.venv\\Lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Sample run for 1 document\n",
    "# For debug, run pipeline.faiss_read() in cell below to prevent rereading doc\n",
    "pipeline = DocumentAnalysis()\n",
    "doc_path = \"../data/1706.03762.pdf\"\n",
    "doc = pipeline.read_from_path(doc_path)\n",
    "\n",
    "# Processing for each page\n",
    "# Remove [:3] for entire doc, keep for testing\n",
    "for page_idx, page in enumerate(doc[:3]):\n",
    "    blocks = pipeline.detect_layout(page)\n",
    "\n",
    "    # Processing for each block to be vectorized\n",
    "    for b in blocks:\n",
    "        # Process as an image if detected type is a figure, else process as text\n",
    "        if b.type == \"Figure\":\n",
    "            # Crop and get image embeddings\n",
    "            segmented_image = pipeline._crop_image(page, b, padding=20)\n",
    "            image_embs = pipeline.encode_image(segmented_image)\n",
    "            pipeline.add_to_faiss(embedding=image_embs, content=\"Figure\", content_type=b.type, page_idx=page_idx, bbox=b.block.coordinates)\n",
    "        else:\n",
    "            # Chunk text and create new blocks, and process for each block\n",
    "            # Returns list even if not chunked\n",
    "            chunks = pipeline.chunk_text(b.text)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                b2 = deepcopy(b)\n",
    "                b2.set(text=chunk, inplace=True)\n",
    "\n",
    "                # Create duplicate blocks for each chunk\n",
    "                # Encode using text and add to FAISS\n",
    "                text_embs = pipeline.encode_text(b2.text)\n",
    "                pipeline.add_to_faiss(embedding=text_embs, content=b2.text, content_type=b.type, page_idx=page_idx, bbox=b2.block.coordinates)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "np.int64(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfaiss_read()\n\u001b[0;32m      4\u001b[0m query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow is the encoder used in the Transformer architecture?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_faiss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 123\u001b[0m, in \u001b[0;36mDocumentAnalysis.search_faiss\u001b[1;34m(self, query, k)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Display retrieved items\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# retrieved items accessed by metadata_store using fetched indices\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata_store\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_store[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Cross encoder reranking on text modality\u001b[39;00m\n\u001b[0;32m    126\u001b[0m answers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_store[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_store[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFigure\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: np.int64(0)"
     ]
    }
   ],
   "source": [
    "# pipeline = DocumentAnalysis()\n",
    "pipeline.faiss_persist()\n",
    "pipeline.faiss_read()\n",
    "query=\"How is the encoder used in the Transformer architecture?\"\n",
    "pipeline.search_faiss(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform retrieval\n",
    "# To implement 2 stage retrieval\n",
    "\n",
    "\n",
    "def search_faiss(query, k=3):\n",
    "    query_embedding = encode_text(query)\n",
    "    _, indices = faiss_index.search(query_embedding, k)\n",
    "    \n",
    "    # Display retrieved items\n",
    "    for idx in indices[0]:\n",
    "        print(f\"Retrieved {metadata_store[idx]['type']}: {metadata_store[idx]['content']}\")\n",
    "\n",
    "    # Cross encoder reranking on text modality\n",
    "    answers = [metadata_store[idx]['content'] for idx in indices[0] if metadata_store[idx]['type']!='Figure']\n",
    "    pairs = [(query, ans) for ans in answers]\n",
    "    scores = model.predict(pairs) # Predict\n",
    "\n",
    "    # Output should be ranked in descending order, so pick top\n",
    "    best_index = np.argmax(scores)\n",
    "    best_answer = answers[best_index]\n",
    "\n",
    "index = faiss.IndexFlatL2(512) # FAISS Vector store\n",
    "metadata = {}\n",
    "\n",
    "query = 'What is the population of Berlin?'\n",
    "anss ['Berlin has a population of 19 million people.', 'berlin is a nice place to live', 'among us']\n",
    "\n",
    "inputs = self.clip_processor(text=[text], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embedding = self.clip_model.get_text_features(**inputs).numpy()\n",
    "return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example: Retrieve images using text query\n",
    "search_faiss(\"solar energy\", query_type=\"text\", k=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
