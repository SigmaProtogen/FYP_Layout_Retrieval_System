[
    {
        "question": "How many parameters does the convolutional neural network have?",
        "answer": {
            "value": "The network consists of 60 million parameters and 650,000 neurons",
            "page": 1,
            "bbox": [
                399.05,
                772.5,
                902.5,
                391.98
            ]
        }
    },
    {
        "question": "Where is Dropout used in the architecture?",
        "answer": {
            "value": "Dropout is applied in the first two fully connected layers of the architecture.",
            "page": 6,
            "bbox": [
                299.3,
                1089.6,
                1099.01,
                56.1
            ]
        }
    },
    {
        "question": "What optimization algorithm was used to train the CNN, and what were its key hyperparameters?",
        "answer": {
            "value": "The model was trained using stochastic gradient descent (SGD) with a batch size of 128, momentum of 0.9, and weight decay of 0.0005.",
            "page": 6,
            "bbox": [
                299.74,
                1293.37,
                640.44,
                209.64
            ]
        }
    },
    {
        "question": "How did the authors handle images of varying resolutions in ImageNet?",
        "answer": {
            "value": "All images were downsampled such that the shorter side was resized to 256 pixels, followed by cropping a 256x256 patch from the center.",
            "page": 2,
            "bbox": [
                300.31,
                1463.8,
                1099.79,
                177.2
            ]
        }
    },
    {
        "question": "What is the role of the Rectified Linear Unit (ReLU) in training deep CNNs?",
        "answer": {
            "value": "ReLU accelerates training by avoiding saturating nonlinearities and allows deep networks to train faster than those using tanh or sigmoid activations.",
            "page": 3,
            "bbox": [
                300.56,
                835.97,
                599.05,
                331.76
            ]
        }
    },
    {
        "question": "Why did the authors use two GPUs for training, and how was the workload distributed?",
        "answer": {
            "value": "One GPU processed half of the convolutional layers, while the other processed the remaining layers, reducing training time and memory constraints.",
            "page": 3,
            "bbox": [
                299.7,
                1303.3,
                1101.98,
                330.02
            ]
        }
    },
    {
        "question": "What property makes a Rectified Linear Unit (ReLU) desirable in learning?",
        "answer": {
            "value": "ReLUs do not require input normalization to prevent values from saturating.",
            "page": 4,
            "bbox": [
                299.6,
                292.37,
                1100.42,
                184.64
            ]
        }
    },
    {
        "question": "How did overlapping pooling contribute to the model's performance?",
        "answer": {
            "value": "Overlapping pooling reduced overfitting by increasing feature generalization while slightly lowering the top-1 and top-5 error rates.",
            "page": 4,
            "bbox": [
                299.4,
                1070.3,
                1100.7,
                299.14
            ]
        }
    },
    {
        "question": "How did the authors perform image augmentation to increase the size of the training set?",
        "answer": {
            "value": "Random patches of 224x224 pixels (with data augmentation applied) were extracted from 256x256 images, increasing the size of the training set by a factor of 2048.",
            "page": 5,
            "bbox": [
                299.7,
                1596.5,
                1100.7,
                269.9
            ]
        }
    },
    {
        "question": "How does dropout help a neural network to learn better?",
        "answer": {
            "value": "Dropout forces neurons to rely on multiple independent features, preventing complex co-adaptations and improving generalization.",
            "page": 6,
            "bbox": [
                300.2,
                677.38,
                1099.9,
                391.56
            ]
        }
    },
    {
        "question": "What hardware was used for training, and for how long?",
        "answer": {
            "value": "Training took five to six days on two GTX 580 3GB GPUs.",
            "page": 2,
            "bbox": [
                299.4,
                1824.92,
                1100.3,
                208.98
            ]
        }
    },
    {
        "question": "What performance did the CNN achieve on the ILSVRC-2010 test set?",
        "answer": {
            "value": "The model achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively.",
            "page": 7,
            "bbox": [
                299.7,
                427.2,
                1099.05,
                177.79
            ]
        }
    },
    {
        "question": "How can similarity computation between high-dimensional vectors made more efficient?",
        "answer": {
            "value": "By training an autoencoder to compress the vectors into short binary codes.",
            "page": 8,
            "bbox": [
                300.56,
                1332.3,
                1099.64,
                147.1
            ]
        }
    },
    {
        "question": "How is the network's learned knowledge examined?",
        "answer": {
            "value": "The feature vectors at the last layer can be compared with other features to calculate the similarity, and similar images tend to have similar feature vectors.",
            "page": 8,
            "bbox": [
                300.28,
                1072.2,
                1099.06,
                239.5
            ]
        }
    },
    {
        "question": "What effect did depth have on network performance?",
        "answer": {
            "value": "Removing any convolutional layer significantly degraded performance, showing that depth is crucial.",
            "page": 8,
            "bbox": [
                300.38,
                1626.09,
                1099.06,
                147.71
            ]
        }
    },
    {
        "question": "How many neurons does each fully connected layer in the model have?",
        "answer": {
            "value": "Each fully connected layer has 4096 neurons.",
            "page": 5,
            "bbox": [
                299.01,
                811.03,
                1100.04,
                208.08
            ]
        }
    },
    {
        "question": "What future improvements did the authors suggest?",
        "answer": {
            "value": "They proposed training even larger CNNs on video sequences to leverage temporal information.",
            "page": 8,
            "bbox": [
                300.39,
                1794.38,
                1099.06,
                239.35
            ]
        }
    },
    {
        "question": "What are the main advantages of convolutional networks over traditional feedforward networks?",
        "answer": {
            "value": "CNNs require fewer connections and parameters due to weight sharing and spatial locality assumptions, making them easier to train.",
            "page": 1,
            "bbox": [
                299.2,
                1732.67,
                1101.02,
                300.9
            ]
        }
    },
    {
        "question": "How were the model weights initialized before training?",
        "answer": {
            "value": "Weights were initialized from a zero-mean Gaussian distribution with standard deviation 0.01",
            "page": 6,
            "bbox": [
                300.1,
                1777.2,
                1100.9,
                149.0
            ]
        }
    },
    {
        "question": "How many images does the ImageNet dataset have??",
        "answer": {
            "value": "The ImageNet dataset contains over 15 million labeled high-resolution images in over 22,000 categories.",
            "page": 2,
            "bbox": [
                299.4,
                1033.47,
                1101.99,
                210.05
            ]
        }
    }
]
