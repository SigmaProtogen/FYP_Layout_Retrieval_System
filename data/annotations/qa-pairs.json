[
    {
        "question": "What is the main contribution of the Transformer model?",
        "answer": {
            "value": "The Transformer model replaces recurrence and convolutions entirely with self-attention mechanisms, allowing for more parallelization and faster training.",
            "page": 1,
            "bbox": ""
        }
    },
    {
        "question": "Which tasks were used to evaluate the Transformer model?",
        "answer": {
            "value": "The Transformer model was evaluated on WMT 2014 English-to-German and English-to-French translation tasks, as well as English constituency parsing.",
            "page": 1,
            "bbox": ""
        }
    },
    {
        "question": "How does self-attention improve computational efficiency compared to RNNs?",
        "answer": {
            "value": "Self-attention eliminates the sequential nature of RNNs, allowing parallelization and reducing the number of sequential operations required for long-range dependencies.",
            "page": 2,
            "bbox": ""
        }
    },
    {
        "question": "What are the main components of the Transformer architecture?",
        "answer": {
            "value": "The Transformer consists of an encoder-decoder structure, with multi-head self-attention, position-wise feed-forward networks, and residual connections with layer normalization.",
            "page": 3,
            "bbox": ""
        }
    },
    {
        "question": "How many layers does the Transformer encoder and decoder have?",
        "answer": {
            "value": "The encoder and decoder both have a stack of N = 6 identical layers.",
            "page": 3,
            "bbox": ""
        }
    },
    {
        "question": "What is the purpose of the residual connections in the Transformer?",
        "answer": {
            "value": "Residual connections help with gradient flow and allow training of deeper networks by summing the sub-layer output with the input before applying layer normalization.",
            "page": 3,
            "bbox": ""
        }
    },
    {
        "question": "How is attention computed in the Transformer?",
        "answer": {
            "value": "Attention is computed as a weighted sum of values, where the weights are derived from scaled dot-product similarity between queries and keys.",
            "page": 4,
            "bbox": ""
        }
    },
    {
        "question": "What is the purpose of the scaling factor in scaled dot-product attention?",
        "answer": {
            "value": "The scaling factor of 1/sqrt(d_k) prevents dot products from growing too large, which would push the softmax function into regions with very small gradients.",
            "page": 4,
            "bbox": ""
        }
    },
    {
        "question": "Why does the Transformer use multi-head attention?",
        "answer": {
            "value": "Multi-head attention allows the model to jointly attend to information from different representation subspaces, improving learning capacity.",
            "page": 5,
            "bbox": ""
        }
    },
    {
        "question": "How many attention heads are used in the Transformer?",
        "answer": {
            "value": "The Transformer model employs h = 8 attention heads.",
            "page": 5,
            "bbox": ""
        }
    },
    {
        "question": "What are the three types of multi-head attention used in the Transformer?",
        "answer": {
            "value": "The Transformer uses encoder-decoder attention, encoder self-attention, and decoder self-attention.",
            "page": 5,
            "bbox": ""
        }
    },
    {
        "question": "What function does the position-wise feed-forward network serve?",
        "answer": {
            "value": "It applies two linear transformations with a ReLU activation in between, allowing non-linearity while keeping computations position-independent.",
            "page": 6,
            "bbox": ""
        }
    },
    {
        "question": "How does the Transformer encode positional information?",
        "answer": {
            "value": "The Transformer uses sinusoidal positional encodings, where different frequencies represent token positions in the sequence.",
            "page": 6,
            "bbox": ""
        }
    },
    {
        "question": "What is one advantage of using sinusoidal positional encodings?",
        "answer": {
            "value": "Sinusoidal encodings allow the model to extrapolate to sequence lengths longer than those seen during training.",
            "page": 6,
            "bbox": ""
        }
    },
    {
        "question": "Why is self-attention preferred over convolutional or recurrent layers?",
        "answer": {
            "value": "Self-attention reduces the number of sequential operations required and allows direct access to information across long sequences.",
            "page": 7,
            "bbox": ""
        }
    },
    {
        "question": "What dataset was used for training the Transformer on machine translation?",
        "answer": {
            "value": "The Transformer was trained on the WMT 2014 English-to-German and English-to-French datasets.",
            "page": 8,
            "bbox": ""
        }
    },
    {
        "question": "What optimizer was used to train the Transformer model?",
        "answer": {
            "value": "The Transformer was trained using the Adam optimizer with β1 = 0.9, β2 = 0.98, and ε = 10^(-9).",
            "page": 9,
            "bbox": ""
        }
    },
    {
        "question": "How long did it take to train the Transformer model?",
        "answer": {
            "value": "The base model was trained for 12 hours, while the big model was trained for 3.5 days on 8 NVIDIA P100 GPUs.",
            "page": 9,
            "bbox": ""
        }
    },
    {
        "question": "How does the Transformer perform compared to previous models?",
        "answer": {
            "value": "The Transformer achieves state-of-the-art BLEU scores on WMT 2014 English-to-German (28.4) and English-to-French (41.8) while requiring significantly less training time.",
            "page": 10,
            "bbox": ""
        }
    },
    {
        "question": "What future improvements were suggested for the Transformer?",
        "answer": {
            "value": "The authors suggested extending the Transformer to handle modalities beyond text and exploring local attention mechanisms for larger inputs like images, audio, and video.",
            "page": 11,
            "bbox": ""
        }
    }
]
